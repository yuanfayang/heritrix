<?xml version="1.0" encoding="UTF-8" ?>
<?xml-stylesheet type="text/xsl" href="wadl_documentation.xsl"?>
<application xmlns="http://research.sun.com/wadl/2006/10"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:html="http://www.w3.org/1999/xhtml"
	xsi:schemaLocation="http://research.sun.com/wadl/2006/10 https://wadl.dev.java.net/wadl20061109.xsd
						http://www.w3.org/1999/xhtml http://www.w3.org/2002/08/xhtml/xhtml1-strict.xsd">
	<doc title="Heritrix REST Web Service (draft)">
    <html:p>The REST web service allows you to control one or more 
    <html:a href="http://crawler.archive.org/">Heritrix</html:a> web crawlers.
    Note that not all functionality is currently available via
    the REST service, you may have to resort to using the JMX API for some
    operations.</html:p>
     
  </doc>
	<resources base="">
		<resource id="org.archive.crawler.restws.CrawlersResource"
			path="crawlers">
			<method name="GET">
				<doc>List the crawlers available through this service.</doc>
				<request>
					<param style="query" name="version">
						<doc>Filter by crawlers running a particular software version, eg <html:tt>Heritrix 2.0.1</html:tt></doc>
					</param>
				</request>
				<response>
					<representation mediaType="text/plain">
						<doc title="Crawler list">Example:<html:pre>c01.example.org:1234:0
c01.example.org:1234:1
c14.example.org:7000:0
...</html:pre></doc>
					</representation>
					<representation mediaType="application/atom+xml">
						<doc title="Crawler list">Example:
							<html:code><html:pre>&lt;feed xmlns="http://www.w3.org/2005/Atom" 
      xml:base="http://example.org/crawlers"&gt;
  &lt;id&gt;http://example.org/crawlers&lt;/id&gt;
  &lt;title&gt;Crawlers known to example.org&lt;/title&gt;
  &lt;updated&gt;2008-08-12T13:40:03Z&lt;/updated&gt;
  &lt;author&gt;&lt;name&gt;Heritrix REST-WS 2.0.1&lt;/name&gt;&lt;/author&gt;
  &lt;entry&gt;
    &lt;id&gt;http://example.org/crawlers/c01.example.org:1234:0&lt;/id&gt;
    &lt;title&gt;c01.example.org:1234:0&lt;/title&gt;
    &lt;link rel="alternate" href="c01.example.org:1234:0"/&gt;
    &lt;link rel="jobs" href="c01.example.org:1234:0/jobs" /&gt;
    &lt;link rel="profiles" href="c01.example.org:1234:0/profiles" /&gt;
    &lt;author&gt;&lt;name&gt;Heritrix 2.2.0&lt;/name&gt;&lt;/author&gt;
    &lt;updated&gt;2006-08-12T13:40:03Z&lt;/updated&gt;
    &lt;summary&gt;2 active jobs: awesomejob, someotherjob. Combined rate: 40kb/s. 
    Uptime: 20 minutes.&lt;/summary&gt;
  &lt;/entry&gt;
  &lt;entry&gt;
     ...
  &lt;/entry&gt;
  ...
&lt;/feed&gt;</html:pre></html:code></doc>
					</representation>
				</response>
			</method>
			<method name="POST">
				<doc>Register a new crawler with the service.</doc>
			</method>
			<method name="DELETE">
				<doc>Unregister a crawler from the service.</doc>
			</method>
			<resource path="available">
				<method name="POST">
					<doc>Find a crawler that is currently available to take on more jobs.  A reference to the crawler
					will be returned in the Location header.  The returned crawler will locked so that it will
					not be returned in subsequent requests to this action for a short period of time.  This
					gives the consumer of the service a period of exclusive access so that it can create and start a
					job without conflicting with other consumers.</doc>
					<request>
						<param style="query" name="version">
							<doc>Filter by crawlers running a particular software version, eg <html:tt>Heritrix 2.0.1</html:tt></doc>
						</param>
						<param style="query" name="locktime" default="10">
							<doc>Number of seconds the crawler will be locked for.</doc>
						</param>
					</request>
				</method>
			</resource>
			<resource id="org.archive.crawler.restws.CrawlerResource"
				path="{crawler}">
				<param style="template" name="crawler">
					<doc>The id (<html:em>host</html:em>:<html:em>port</html:em>:<html:em>identifier</html:em>) of a crawler.</doc>
				</param>
				<method name="GET">
					<response>
					<!-- TODO: something -->
						<fault status="404">
							<doc title="Unknown crawler">The specified crawler is unknown to the service.</doc>
						</fault>
						<fault status="400">
							<doc title="Malformed crawler id">The specified crawler id is not in the required <html:em>host</html:em>:<html:em>port</html:em>:<html:em>identifier</html:em> form.</doc>
						</fault>
					</response>
				</method>
				<resource id="org.archive.crawler.restws.JobsResource"
					path="jobs">
					<method name="GET">
						<doc>List all crawl jobs.
						
						
						</doc>
						<response>
							<representation mediaType="text/plain">
								<doc title="Job list">Example:<html:pre>awesomejob
anotherjob
someotherjob
...</html:pre></doc>
							</representation>
							<representation mediaType="application/atom+xml">
								<doc title="Job list">Example:
							<html:code><html:pre>&lt;feed xmlns="http://www.w3.org/2005/Atom" 
      xml:base="http://example.org/crawlers/c01.example.org:1234:0/jobs"&gt;
  &lt;id&gt;http://example.org/crawlers/c01.example.org:1234:0/jobs&lt;/id&gt;
  &lt;title&gt;Jobs on c01.example.org&lt;/title&gt;
  &lt;updated&gt;2008-08-12T13:40:03Z&lt;/updated&gt;
  &lt;author&gt;&lt;name&gt;Heritrix 2.0.1&lt;/name&gt;&lt;/author&gt;
  &lt;entry&gt;
    &lt;id&gt;http://example.org/crawlers/c01.example.org:1234:0/jobs/awesomejob&lt;/id&gt;
    &lt;title&gt;awesomejob&lt;/title&gt;
    &lt;link rel="alternate" href="awesomejob"/&gt;
    &lt;link rel="seeds" href="awesomejob/seeds" /&gt;
    &lt;link rel="sheets" href="awesomejob/sheets" /&gt;
    &lt;updated&gt;2006-08-12T13:40:03Z&lt;/updated&gt;
    &lt;summary&gt;My awesome job. [20s elapsed. 208 KB received.]&lt;/summary&gt;
  &lt;/entry&gt;
  &lt;entry&gt;
     ...
  &lt;/entry&gt;
  ...
&lt;/feed&gt;</html:pre></html:code></doc></representation>
						</response>
					</method>
					<resource id="org.archive.crawler.restws.JobResource"
						path="{job}">
						<param style="template" name="job">
							<doc>The name of a job.</doc>
						</param>
						<method name="GET">
							<doc>Get information about the job.<html:p>Retrieve an XML or JSON document with the 
							status of the job and crawl statistics.</html:p>
							</doc>
							<response>
								<representation mediaType="application/xml">
									<doc title="Job">
									<html:p>Example:</html:p>
							<html:code><html:pre>&lt;job&gt;
  &lt;name&gt;testjob-20080702&lt;/name&gt;
  &lt;status&gt;RUNNING&lt;/status&gt;
  &lt;stats&gt;
    &lt;elapsedMillis&gt;40202&lt;/elapsedMillis&gt;
    &lt;totalBytes&gt;2033211&lt;/totalBytes&gt;
    &lt;bytesPerSecond&gt;202&lt;/bytesPerSecond&gt;
    ...  
  &lt;/stats&gt;

&lt;job&gt;
					  
							</html:pre></html:code>
									</doc>
								</representation>
								<fault status="404">
									<doc title="Job not found">
									The job does not exist or has already been deleted.
								</doc>
								</fault>
							</response>
						</method>
						<method name="POST">
							<doc>Copy this job to create a new job or profile.</doc>
							<request>
								<representation mediaType="multipart/form-data">
									<doc title="Copy job request" />
									<param style="plain" name="name">
										<doc>The name of the new job</doc>
									</param>
									<param style="plain" name="stage">
										<doc>The type of job: a normal job <html:em>ready</html:em> to be launched or
									a <html:em>profile</html:em>.</doc>
										<option value="ready"></option>
										<option value="profile"></option>
									</param>
								</representation>
							</request>
							<response>
								<representation status="201">
									<doc title="Job created">
									The job was created successfully.
								</doc>
									<param style="header" name="Location">
										<doc>URL of the created job.</doc>
									</param>
								</representation>
								<fault status="409">
									<doc title="Target already exists">
									A job with the name you specified already exists.  Try a different name.
								</doc>
								</fault>
							</response>
						</method>
						<method name="DELETE">
							<doc>Delete the job.</doc>
							<response>
								<representation status="204">
									<doc title="Job deleted">
									The job was deleted successfully.
								</doc>
								</representation>
							</response>
						</method>
						<resource id="org.archive.crawler.restws.SeedsResource"
							path="seeds">
							<doc>
							The seed list is the initial set of URLs that crawling will begin from.
						</doc>
							<method name="GET">
								<doc>Get the list of seed URLs.</doc>
								<response>
									<representation status="200" mediaType="text/plain">
										<doc title="Seeds list">
										<html:p>The list of seeds is simply plain text URLs delimited by linefeeds.</html:p>
										<html:p>Example:
										<html:pre>http://www.archive.org/
http://www.example.com/
http://example.org/mypage.html</html:pre></html:p>
									</doc>
									</representation>
								</response>
							</method>
							<method name="POST">
								<doc>Append some new seed URLs.</doc>
								<request>
									<representation mediaType="text/plain">
										<doc title="Append seeds request">
										The seed URL desired to be appended should be delimited by linefeeds as above and POSTed as text/plain.
									</doc>
									</representation>
								</request>
							</method>
							<method name="PUT">
								<doc>Replace the list of seed URLs.</doc>
								<request>
									<representation mediaType="text/plain">
										<doc title="Replace seeds request">
										The seed list should be delimited by linefeeds as above and PUT as text/plain.
									</doc>
									</representation>
								</request>
							</method>
						</resource>
						<resource id="org.archive.crawler.restws.SheetsResource"
							path="sheets">
							<doc><html:p>A configuration sheet is a list of settings that control the crawl. By default, there is only one 
						sheet, the global sheet. The global sheet is consulted when no URL-specific configuration information is needed.
						Other sheets of settings can be created, and these extra sheets can be associated with URLs to provide crawl
						configuration specific to those URLs.</html:p>
							<html:p>Some crawlers may only support the global sheet.</html:p>
						</doc>
							<method name="GET">
								<doc>List the available sheets.</doc>
								<response>
									<representation status="200" mediaType="text/plain"></representation>
									<representation status="200" mediaType="application/xml"></representation>
								</response>
							</method>
							<resource id="org.archive.crawler.restws.SheetResource"
								path="{sheet}">
								<doc><html:p>The actual configuration format is specific to different crawlers and even different versions of crawlers.</html:p>
							<html:p>In Heritrix 2.0, sheets are plain text files that look like this:</html:p>
							<html:code><html:pre>root=map, java.lang.Object
root:metadata=primary, org.archive.modules.writer.DefaultMetadataProvider
root:metadata:robots-honoring-policy=primary, org.archive.modules.net.RobotsHonoringPolicy
root:metadata:robots-honoring-policy:user-agents=list, java.lang.String
root:metadata:description=string, Broad but shallow crawl.
root:loggerModule=primary, org.archive.crawler.framework.CrawlerLoggerModule
[...]</html:pre></html:code>
<html:p>In future versions, beginning with Heritrix 2.2, the configuration system is being redesigned to use Spring XML bean configuration.  
How this will work has not been finalized, but it is likely the REST service will just be able to return chunks of bean XML for each sheet.</html:p>
							</doc>
								<param style="template" name="sheet">
									<doc>The name of a configuration sheet.</doc>
								</param>
								<method name="GET">
									<doc>Get the sheet's configuration.</doc>
								</method>
								<method name="PUT">
									<doc>Set the sheet's configuration.  If the sheet did not previously exist, it will be created.</doc>
								</method>
								<method name="DELETE">
									<doc>Delete the sheet.</doc>
								</method>
								<resource id="org.archive.crawler.restws.SheetSurtsResource"
									path="surts">
									<doc><html:p>Associated with each sheet can be a list of <html:a
										href="http://crawler.archive.org/articles/user_manual/glossary.html#surtprefix">SURT prefixes</html:a>
									which define the URLs the sheet applies to.</html:p></doc>
									<method name="GET">
										<doc>Return the list of SURT prefixes associated with this sheet.</doc>
									</method>
									<method name="POST">
										<doc>Append some new SURT prefixes to this sheet.</doc>
									</method>
									<method name="PUT">
										<doc>Replace the list of SURT prefixes associated with this sheet.</doc>
									</method>
								</resource>
							</resource>
						</resource>
						<resource id="org.archive.crawler.restws.JobStatusResource"
							path="status">
							<doc>The status of the job.  Possible values include:
						<html:ul>
						  <html:li>NASCENT</html:li>
						  <html:li>PREPARING</html:li>
						  <html:li>RUNNING</html:li>
						  <html:li>PAUSING</html:li>
						  <html:li>PAUSED</html:li>
						  <html:li>CHECKPOINTING</html:li>
						  <html:li>STOPPING</html:li>
						  <html:li>FINISHED</html:li>
						  </html:ul>
        				</doc>
							<method name="GET">
								<doc>Get the current status of the job.</doc>
							</method>
							<method name="PUT">
								<doc>
        				<html:p>Change the status of the job.  Only some status transitions are valid, others will return an error.</html:p>
        				<html:p>To launch a NASCENT job, set the status to PREPARING.</html:p>
        				<html:p>To pause a RUNNING or CHECKPOINTING job, set the status to PAUSING.</html:p>
        				<html:p>To resume a PAUSING, PAUSED or CHECKPOINTING job, set the status to RUNNING.</html:p>
        				<html:p>To <html:a
									href="http://crawler.archive.org/articles/user_manual/outside.html#checkpoint">checkpoint</html:a>
        								a RUNNING, PAUSING or PAUSED job, set the status to CHECKPOINTING.</html:p>
        				<html:p>To stop a job with any status, set the status to STOPPING.</html:p>    					
        				</doc>
								<response>
									<fault status="400">
										<doc title="Unknown status">You attempted to assign the job
        						a status which this crawler does not understand.</doc>
									</fault>
									<fault status="409">
										<doc title="Invalid status transition">The job cannot transition
        						directly from its current status to the one you requested.</doc>
									</fault>
								</response>
							</method>
						</resource>
						<resource id="org.archive.crawler.restws.ReportsResource"
							path="reports">
							<doc>Some crawlers provide reports while the crawl is ongoing on more commonly once is has finished.  These
						reports often include statistics about the types of files downloaded.</doc>
							<method name="GET">
								<doc>List the available reports.</doc>
							</method>
							<method name="POST">
								<doc>Trigger report regeneration.  This action may not be supported by all crawlers and if it
							is supported it may be valid only when the job is in a particular state.</doc>
							</method>
							<resource id="org.archive.crawler.restws.ReportResource"
								path="{report}">
								<param style="template" name="report">
									<doc>The filename of a report.</doc>
								</param>
								<method name="GET">
									<doc>Retrieve the contents of the report.</doc>
								</method>
							</resource>
						</resource>
					</resource>
				</resource>
				<resource path="profiles">
					<doc>Profiles act as templates for jobs.  New jobs are created by copying their configuration from an
				existing profile.</doc>
					<method name="GET">
						<doc>List the profiles known to this crawler.</doc>
					</method>
					<resource path="{profile}">
						<doc>See <html:a href="##org.archive.crawler.restws.JobResource">job</html:a>.</doc>
						<param style="template" name="profile">
							<doc>The name of a profile.</doc>
						</param>
					</resource>
				</resource>
			</resource>
		</resource>
		<resource id="org.archive.crawler.restws.WadlResource" path="/wadl.xml">
			<method name="GET">
				<doc>Return <html:a href="https://wadl.dev.java.net/">WADL</html:a> 
			documentation for the serviceâ€”an online version of
			this document.  Error messages link to this.</doc>
			</method>
		</resource>
		<resource id="org.archive.crawler.restws.WadlXslResource"
			path="/wadl_documentation.xsl">
			<method name="GET">
				<doc>Return an XSLT stylesheet for styling the WADL documentation as XHTML.
			</doc>
			</method>
		</resource>
	</resources>
</application>