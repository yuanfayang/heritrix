<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">
<article>
  <title>Heritrix User Manual</title>

  <sect1>
    <title>Introduction</title>

    <para>Heritrix is the Internet Archive's open-source, extensible,
    web-scale, archival-quality web crawler.</para>

    <para>This document explains how to create, configure and run crawls using
    it. It is intended for users of the software and presumes that they
    possess at least a general familiarity with the concept of web
    crawling.</para>

    <para>If you want to build Heritrix from source or if you'd like to make
    contributions and would like to know about contribution conventions, etc.,
    see instead the Developer Manual.</para>
  </sect1>

  <sect1>
    <title>Installing and running Heritrix</title>

    <para>This chapter will explain how to set up Heritrix. Some basic Linux
    administration skills are needed for this.</para>

    <para>Because Heritrix is a pure Java program it can (in theory anyway) be
    run on any platform that has a Java 1.4 VM. However we are only committed
    to supporting it's operation on Linux and so this chapter only covers
    setup on that platform. Other chapters in the user manual are entirely
    platform agnostic.</para>

    <para>This chapter also only covers installing and running the prepackaged
    binary distributions of Heritrix. For information about downloading and
    compiling the source see the Developers manual.</para>

    <sect2>
      <title>Obtaining and installing Heritrix</title>

      <para>The packaged binary can be downloaded from the project's <ulink
      url="http://sourceforge.net/projects/archive-crawler">sourceforge home
      page</ulink>. Each release comes in four flavors, packaged as .tar.gz or
      .zip and including source or not.</para>

      <para>For installation on Linux get the file
      <filename>heritrix-?.?.?.tar.gz</filename> (where ?.?.? is the most
      recent version number).</para>

      <para>The packaged binary comes largely ready to run. Once downloaded it
      can be untarred into the desired directory.</para>

      <para><programlisting>  % tar xfz heritrix-?.?.?.tar.gz</programlisting></para>

      <para>Once you have downloaded and untarred the correct file you can
      move on to the next step.</para>

      <sect3>
        <title>System requirements</title>

        <sect4>
          <title>Java Runtime Environment </title>

          <para>The Heritrix crawler is implemented purely in Java. This means
          that the only true requirement for running it is that you have a JRE
          installed. </para>

          <para>The Heritrix crawler makes use of Java 1.4 features so your
          JRE must be at least of a 1.4.0 pedigree.</para>

          <para>We currently include all of the free/open source third-party
          libraries necessary to run Heritrix in the distribution package. See
          <ulink
          url="http://crawler.archive.org/dependencies.html">dependencies</ulink>
          for the complete list (Licenses for all of the listed libraries are
          listed in the dependencies section of the raw project.xml at the
          root of the <literal>src</literal> download or on Sourceforge).
          </para>

          <sect5>
            <title>Installing Java</title>

            <para>If you do not have Java installed you can download Java
            from:</para>

            <itemizedlist>
              <listitem>
                <para><emphasis role="bold">Sun</emphasis> -- <ulink
                url="http://java.sun.com/">java.sun.com</ulink></para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">IBM</emphasis> -- <ulink
                url="http://www.ibm.com/java">www.ibm.com/java</ulink></para>
              </listitem>
            </itemizedlist>

            <para>It has been our experience that running on Linux with the
            IBM Java virtual machine Heritrix performs better then with the
            Sun JavaVM (Java version 1.4.2). This may change in the future.
            There are no known problems with using either of the two.</para>
          </sect5>
        </sect4>

        <sect4>
          <title>Hardware</title>

          <para> We recommend assigning at least 256MB RAM to the Java heap
          (via the "-Xmx256MB" VM argument), which is usually suitable for
          crawls that range over hundreds of hosts.</para>
        </sect4>

        <sect4>
          <title>Linux</title>

          <para>The Heritrix crawler has been built and tested primarily on
          Linux. It has seen some informal use on Macintosh, Windows 2000 and
          Windows XP, but is not tested, packaged, nor supported on platforms
          other than Linux at this time.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2>
      <title>Running Heritrix</title>

      <para>To run Heritrix, first do the following:<programlisting>  % export HERITRIX_HOME=/PATH/TO/BUILT/HERITRIX</programlisting>...where
      <literal>$HERITRIX_HOME</literal> is the location of your untarred
      <filename>heritrix.?.?.?.tar.gz</filename>.</para>

      <para>Next run:<programlisting>  % cd $HERITRIX_HOME
  % chmod u+x $HERITRIX_HOME/bin/heritrix
  % $HERITRIX_HOME/bin/heritrix --help</programlisting>This should give you
      usage output like the following:</para>

      <para><programlisting><computeroutput>  Usage: heritrix --help
  Usage: heritrix --nowui ORDER_FILE
  Usage: heritrix [--port=PORT] [--admin=LOGIN:PASSWORD] [--run] [ORDER_FILE]
  Usage: heritrix [--port=PORT] --selftest[=TESTNAME]
  Version: @VERSION@
  Options:
   -a,--admin      Login and password for web user interface administration.
                   Default: admin/letmein.
   -h,--help       Prints this message and exits.
   -n,--nowui      Put heritrix into run mode and begin crawl using ORDER_FILE. Do
                   not put up web user interface.
   -p,--port       Port to run web user interface on.  Default: 8080.
   -r,--run        Put heritrix into run mode. If ORDER_FILE begin crawl.
   -s,--selftest   Run the integrated selftests. Pass test name to it only (Case
                   sensitive: E.g. pass 'Charset' to run charset selftest).
  Arguments:
   ORDER_FILE     Crawl order to run.</computeroutput></programlisting>Launch
      the crawler with the UI enabled by doing the following:</para>

      <para><programlisting>  % $HERITRIX_HOME/bin/heritrix</programlisting>This
      will start up heritrix printing out a startup message that looks like
      the following:</para>

      <para><programlisting>  [b116-dyn-60 619] heritrix-0.4.0 &gt; ./bin/heritrix
  Tue Feb 10 17:03:01 PST 2004 Starting heritrix...
  Tue Feb 10 17:03:05 PST 2004 Heritrix 0.4.0 is running.
  Web UI is at: http://b116-dyn-60.archive.org:8080/admin
  Login and password: admin/letmein</programlisting></para>

      <para>See <xref linkend="wui" /> and <xref linkend="tutorial" /> to get
      your first crawl up and running.</para>

      <sect3>
        <title>Command line options</title>

        <para>A quick overview of the most useful command line options. It is
        not necessary to specify any command line options to run the
        crawler.</para>

        <sect4>
          <title>--port=PORT</title>

          <para><programlisting>  --port=PORT</programlisting></para>

          <para>Set what port the web based user interface runs on. By default
          this is port <literal>8080</literal>.</para>
        </sect4>

        <sect4>
          <title>--admin=LOGIN:PASSWORD</title>

          <para><programlisting>  --admin=LOGIN:PASSWORD</programlisting></para>

          <para>Change the default admin username and password. If you do not
          do this then the default username and password will be in effect.
          Since they are widely known that may not be desirable. Default
          username is <literal>admin</literal> and the default password is
          <literal>letmein</literal>.</para>
        </sect4>
      </sect3>

      <sect3>
        <title>Environment variables</title>

        <sect4>
          <title>HERITRIX_HOME</title>

          <para>Set this environment variable to point at the Heritrix home
          directory. For example, if you've unpacked Heritrix in your home
          directory and Heritrix is sitting in the heritrix-0.5.2 directory,
          you'd set HERITRIX_HOME as follows. Assuming your shell is
          bash:<programlisting>  % export HERITRIX_HOME=~/heritrix-0.5.2</programlisting>If
          you don't set this environment variable, the Heritrix start script
          makes a guess at the home for Heritrix. It doesn't always guess
          correctly.</para>
        </sect4>

        <sect4>
          <title>JAVA_HOME</title>

          <para>This environment variable may already exist. It should point
          to the Java installation on the machine. An example of how this
          might be set (assuming your shell is bash):</para>

          <para><programlisting>  % export JAVA_HOME=/usr/local/java/jre/</programlisting></para>
        </sect4>

        <sect4>
          <title>JAVA_OPTS</title>

          <para>Pass options to the Heritrix JVM by populating the JAVA_OPTS
          environment variable with values. For example, if you want to have
          Heritrix run with a larger heap, say 512 megs, you could do either
          of the following (assuming your shell is bash):<programlisting>  % export JAVA_OPTS="-Xmx512m"
  % $HERITRIX_HOME/bin/heritrix</programlisting>Or, you could do it all on the
          one line as follows:<programlisting>  % JAVA_OPTS="-Xmx512m" $HERITRIX_HOME/bin/heritrix</programlisting></para>
        </sect4>
      </sect3>

      <sect3>
        <title>System properties</title>

        <para>Below we document the system properties passed on the
        command-line that can influence Heritrix behavior.</para>

        <sect4>
          <title>heritrix.development</title>

          <para>Set this property on the command-line (i.e.
          "-Dheritrix.development" or include as part of
          <literal>JAVA_OPTS</literal> environment value) when you want to run
          the crawler from eclipse. When this property is set, the
          <literal>conf</literal> and <literal>webapps</literal> directories
          will be found in their development locations and startup messages
          will show on the console.</para>
        </sect4>

        <sect4>
          <title>javax.net.ssl.trustStore</title>

          <para>Heritrix has its own trust store at
          <literal>conf/heritrix.cacerts</literal> that it uses if the
          <literal>FetcherHTTP</literal> is configured to use a trust level of
          other than open ( open is the default setting). In the unusual case
          where you'd like to have Heritrix use an alternate truststore, point
          at the alternate by supplying the JSSE
          <literal>javax.net.ssl.trustStore</literal> property on the command
          line: e.g.</para>

          <para><programlisting>  java -Djavax.net.ssl.trustStore=/tmp/truststore org.archive.crawler.Heritrix</programlisting></para>
        </sect4>

        <sect4>
          <title>java.util.logging.config.file</title>

          <para>The Heritrix <filename><literal>conf</literal></filename>
          directory includes a file named
          <filename>heritrix.properties</filename> . A section of this file
          specifies the default Heritrix logging configuration. To override
          these settings, point
          <literal>java.util.logging.config.file</literal> at a properties
          file with an alternate logging configuration. Below we reproduce the
          default <filename>heritrix.properties</filename> for
          reference:<programlisting>  # Basic logging setup; to console, all levels
  handlers= java.util.logging.ConsoleHandler
  java.util.logging.ConsoleHandler.level= ALL

  # Default global logging level: only warnings or higher
  .level= WARNING

  # currently necessary (?) for standard logs to work
  crawl.level= INFO
  runtime-errors.level= INFO
  uri-errors.level= INFO
  progress-statistics.level= INFO
  recover.level= INFO

  # HttpClient is too chatty... only want to hear about severe problems
  org.apache.commons.httpclient.level= SEVERE</programlisting>Here's an
          example of how you might specify an override:<programlisting>  % JAVA_OPTS="-Djava.util.logging.config.file=heritrix.properties" \
          ./bin/heritrix --no-wui order.xml</programlisting></para>
        </sect4>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="wui">
    <title>Web based user interface</title>

    <para>After Heritrix has been launched from the command line, the web
    based user interface (WUI) becomes accessible.</para>

    <para>The path to it printed out on the console from which the program was
    launched (typically http://&lt;host&gt;:8080/admin/).</para>

    <para>The WUI is password protected. The username is currently hardcoded
    to 'admin' with the default password of 'letmein'. The password can be
    specified at startup and users are encouraged to change it. The currently
    valid password will be printed out to the console along with the
    path.</para>

    <para>The WUI can then be accessed via any browser. While we've
    endeavoured to make certain that it functions in all recent browsers,
    Mozilla 5+ is recommended. IE 6+ should also work without problems.</para>

    <para>The initial login page takes the standard username/password
    combination discussed before. In addition users can opt to have the WUI
    remember their login by checking the appropriate box. If that is done then
    the login information will be stored in a cookie and read from it during
    future login attempts. If this option is not checked then the login is
    only valid as long as the server's session. Once it times you, the user
    will need to login again.</para>

    <caution>
      <para>The access control to the WUI is not encrypted! Passwords will be
      submitted over the network in plain text.</para>
    </caution>
  </sect1>

  <sect1 id="tutorial">
    <title>A quick guide to running your first crawl job</title>

    <para></para>
  </sect1>

  <sect1>
    <title>Creating jobs and profiles</title>

    <para>In order to run a crawl a configuration must be created that defines
    it. In Heritrix such a configuration is called a Crawl job.</para>

    <sect2>
      <title id="crawljob">Crawl job</title>

      <para>A Crawl job encompasses the configurations needed to run a single
      crawl. It also contains some additional elements such as logs, status
      etc.</para>

      <para>Once logged onto the WUI new jobs can be created by going to the
      <emphasis>Jobs</emphasis> tab. Once the Jobs page loads users can create
      jobs by choosing of the following three options:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Create new crawl job</emphasis></para>

          <para>This option creates a new crawl job based on the currently set
          default profile.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Create new crawl job based on a
          profile</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          one of the profiles offered up.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Create new crawl job based on an
          existing job</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          any existing job, regardless of whether it has been crawled or not.
          Can be useful for repeating crawls..</para>
        </listitem>
      </orderedlist>

      <para>Options 2 and 3 will display a list of available options.
      Initially there is only one profile and no existing jobs.</para>

      <para>As should be clear by now, all crawl jobs are created by basing
      them on profiles (see next chapter) or existing jobs.</para>

      <para>Once the proper profile/job has been chosen to base the new job
      one a simple page will appear asking for the new jobs:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Name</emphasis></para>

          <para>The name must only contain letters, numbers, dash (-) and
          underscore (_). No other characters are allowed. This name will be
          used to identify the crawl in the WUI but it need not be unique. The
          name can not be changed later</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Description</emphasis></para>

          <para>A short description of the job. This can be edited
          later.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Seeds</emphasis></para>

          <para>The seed URLs to use for the job. This list can be edited
          later along with the general configurations.</para>
        </listitem>
      </orderedlist>

      <para>Below these input fields there are several buttons. The last one
      Submit job will immediately submit the job and (assuming that it's
      properly configured) it will be ready to run (see <xref
      linkend="running" />). The other buttons will take the user to the
      relevant configuration pages (those are covered in detail in <xref
      linkend="config" />). Once all desired changes have been made to the
      configuration, click the '<emphasis>Submit job</emphasis>' tab (usually
      displayed top and bottom right) to submit it to the list of waiting
      jobs.<note>
          <para>Changes made afterwards to the original jobs or profiles that
          a new job is based on will <emphasis role="bold">not</emphasis> in
          any way affect the newly created job.</para>
        </note></para>
    </sect2>

    <sect2>
      <title id="profile">Profile</title>

      <para>It's best to think of a profile as a template for a crawl job. It
      contains all the configurations that a crawl job would, but is not
      considered to be 'crawlable'. That is Heritrix will not allow you to
      directly crawl a profile, only jobs based on profiles. The reason for
      this is that while profiles may in fact be complete, they may not
      be.</para>

      <para>A common example is leaving the HTTP headers
      (<emphasis>user-agent</emphasis>, <emphasis>from</emphasis>) in an
      illegal state in a profile to force the user to input valid data. This
      applies to the default (<emphasis>Simple</emphasis>) profile that comes
      with Heritrix. Other examples would be leaving the seeds list empty, not
      specifying some processors (such as the writer/indexer) etc.</para>

      <para>In general there is less error checking of profiles.</para>

      <para>To manage profiles, go to the <emphasis>Profiles</emphasis> tab in
      the WUI. That page will display a list of existing profiles. To create a
      new profile select the option of creating a "New profile based on it"
      from the existing profile to use as a template. Much like jobs, profiles
      can only be created based on other profiles. It is not possible to
      create profiles based on existing jobs.</para>

      <para>The process from there on mirrors the creating of jobs. A page
      will ask for the new profiles name, description and seeds list. Unlike
      job names, profile names <emphasis>must be unique</emphasis> from other
      profile names - jobs and a profile can share the same name - otherwise
      the same rules apply.</para>

      <para>The user then proceeds to the configuration pages (see <xref
      linkend="config" />) to modify the behavior of the new profile from that
      of the parent profile.<note>
          <para>Even though profiles are based on other profiles, changes made
          to the original profiles afterwards will <emphasis
          role="bold">not</emphasis> affect the new ones.</para>
        </note></para>
    </sect2>
  </sect1>

  <sect1 id="config">
    <title>Configuring jobs and profiles</title>

    <para>Creating crawl jobs (<xref linkend="crawljob" />) and profiles
    (<xref linkend="profile" />) is just the first step. Configuring them is a
    more complicated process.</para>

    <para>The following section applies equally to configuring crawl jobs and
    profiles. It does not matter if new ones are being created or existing
    ones are being edited. The interface is almost entirely the same, only the
    '<emphasis>Submit job</emphasis>'/'<emphasis>Finished</emphasis>' button
    will vary slightly.</para>

    <para>Each page in the configuration section of the WUI will have a
    secondary row of tabs below the system ones. This secondary row is often
    replicated at the bottom of longer pages.</para>

    <para>This row offers access to different parts of the configuration.
    While configuring the global level (more on global vs. overrides and
    refinements in <xref linkend="overrides" /> and <xref
    linkend="refinements" />) the following options are available:</para>

    <itemizedlist>
      <listitem>
        <para>Modules (<xref linkend="modules" />)</para>

        <para>Add/remove/set configurable modules</para>
      </listitem>

      <listitem>
        <para>Filters (<xref linkend="filters" />)</para>

        <para>Add/remove/reorder filters</para>
      </listitem>

      <listitem>
        <para>Credentials (<xref linkend="credentials" />)</para>

        <para>Add/remove login credentials</para>
      </listitem>

      <listitem>
        <para>Settings (<xref linkend="settings" />)</para>

        <para>Configure settings on Heritrix modules</para>
      </listitem>

      <listitem>
        <para>Overrides (<xref linkend="overrides" />)</para>

        <para>Override settings on Heritrix modules based on domain</para>
      </listitem>

      <listitem>
        <para>Refinements (<xref linkend="refinements" />)</para>

        <para>Refine settings on Heritrix modules based on arbitrary
        criteria</para>
      </listitem>

      <listitem>
        <para>Submit job / Finished</para>

        <para>Clicking this tab will take the user back to the Jobs or
        Profiles page, saving any changes.</para>
      </listitem>
    </itemizedlist>

    <para>The 'Settings' tab is probably the most frequently used page as it
    allows the user to fine tune the settings of any Heritrix module used in
    the job/profile.</para>

    <para>It is safe to navigate between these, it will not cause new jobs to
    be submitted to the queue of pending jobs. That only happens once the
    Submit job tab is clicked. Navigating out of the configuration pages using
    the top level tabs will cause new jobs to be lost. Any changes made are
    saved when navigating within the configuration pages. There is no undo
    function, once made, changes can not be undone.</para>

    <sect2 id="modules">
      <title>Modules</title>

      <para>Heritrix has several types of pluggable modules. These modules,
      while having a fixed interface and a variable number of provided
      implementation can also be third party plugins. This page allows the
      user to select several types of these pluggable modules.</para>

      <para>Once modules have been added to the configuration they can be
      configured in greater detail on the Settings page (<xref
      linkend="settings" />).</para>

      <para><note>
          <para>Modules are referred to by their Java class names
          (org.archive.crawler.frontier.Frontier). This is done because these
          are the only names we can be assured of being unique.</para>
        </note>See Developer manual for information about creating and adding
      custom modules to Heritrix.</para>

      <sect3>
        <title>URI Frontier</title>

        <para>The URI Frontier is a module that maintains the internal state
        of the crawl. What URIs have been discovered, crawled etc. As such
        it's selection greatly affects for instance the order in which
        discovered URIs are crawled.</para>

        <para>There is only one Frontier per crawl.</para>

        <para>At present only one Frontier is provided with Heritrix. It
        offers some options to influence it between site first and breadth
        first crawling.</para>
      </sect3>

      <sect3 id="scopes">
        <title>Crawl Scope</title>

        <para>A crawl scope is an object that decides for each discovered URI
        if it is within the scope of the current crawl.</para>

        <para>Several scopes are provided with Heritrix:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">BroadScope</emphasis></para>

            <para>This scope allows for limiting the depth of a crawl (how
            many links away Heritrix should crawl) but does not impose any
            limits on which domains or hosts discovered URIs must belong
            to.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">DomainScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of domains
            defined by the provided seeds. That is any URI discovered
            belonging to a domain from which one of the seed came is within
            scope. Like always it is possible to apply depth
            restrictions.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">HostScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of hosts
            defined by the provided seeds.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PathScope</emphasis></para>

            <para>This scope goes yet further and limits the discovered URIs
            to a section of paths on hosts defined by the seeds. Of course any
            host that has a seed pointing at it's root (i.e.
            www.sample.com/index.html) will be included in full where as a
            host whose only seed is www.sample2.com/path/index.html will be
            limited to URIs under /path/.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">FilterScope</emphasis></para>

            <para>A highly configurable scope. By adding different filters in
            different combinations this scope can be configured to provide a
            wide variety of behaviour.</para>
          </listitem>
        </itemizedlist>

        <para>Scopes usually allow for some flexibility in defining depth and
        possible transitive includes (that is getting items that would usually
        be out of scope because of special circumstance such as them being
        embedded). Most noticeably they can have filters applied to themselves
        in two different contexts (some filters may only have one these
        contexts).</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Focus</emphasis></para>

            <para>URIs matching these filters will be considered to be within
            scope</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Exclude</emphasis></para>

            <para>URIs matching these filters will be considered to be out of
            scope.</para>
          </listitem>
        </orderedlist>

        <para>Custom made Scopes may have different set of filters. Also some
        scopes have filters hardcoded into them. This allows you to edit their
        settings but not remove or replace them. For example most of the
        provided scopes have a <literal>Transclusion</literal> filter
        hardcoded into them that handles transitive items (URIs that normally
        shouldn't be included but because of special circumstance they will be
        included).</para>

        <para>For more about Filters see <xref linkend="filters" />.</para>

        <sect4 id="scopeproblems">
          <title>Problems with the current Scopes</title>

          <para>Up until now, our predefined Scope classes -- PathScope,
          HostScope, DomainScope, BroadScope -- all could be thought of as
          fitting a specific pattern: A URI is included if and only if:</para>

          <para><programlisting>    ( ( focusFilter.accepts(u)
        || transitiveFilter.accepts(u) )
      &amp;&amp; exclusionFilter.accepts(u) == false )</programlisting></para>

          <para>More generally, the <emphasis>focus</emphasis> filter was
          meant to rule things in by prima facia/regexp-pattern analysis; the
          <emphasis>transitive</emphasis> filter rule extra items in by
          dynamic path analysis (for example, off site embedded images); and
          the <emphasis>exclusion</emphasis> filter rule things out by any
          number of chained exclusion rules. So in a typical crawl, the
          <emphasis>focus</emphasis> filter drew from one of these
          categories:<itemizedlist>
              <listitem>
                <para><emphasis role="bold">broad</emphasis> : accept
                all</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">domain</emphasis>: accept if on
                same 'domain' (for some definition) as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">host</emphasis>: accept if on
                exact host as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">path</emphasis>: accept if on same
                host and a shared path-prefix as seeds</para>
              </listitem>
            </itemizedlist>The <emphasis>transitive</emphasis> filter was
          configured based on the various link-hops and embed-hops thresholds
          set by the operator.</para>

          <para>The <emphasis>exclusion</emphasis> filter was in fact a
          compound chain of filters, OR'ed together, such that any one of them
          could knock a URI out of consideration. However, a number of aspects
          of this arrangement have caused problems: <orderedlist>
              <listitem>
                <para>To truly understand what happens to an URI, you must
                understand the above nested boolean-construct.</para>
              </listitem>

              <listitem>
                <para>Adding mixed focuses -- such as all of this one host,
                all of this other domain, and then just these paths on this
                other host -- is not supported, nor easy to mix-in to the
                <emphasis>focus</emphasis> filter.</para>
              </listitem>

              <listitem>
                <para>Constructing and configuring the multiple filters
                required many setup steps across several WUI pages.</para>
              </listitem>

              <listitem>
                <para>The reverse sense of the <emphasis>exclusion</emphasis>
                filters -- if URIs are accepted by the filter, they are
                excluded from the crawl -- proved confusing, exacerbated by
                the fact that 'filter' itself can commonly mean either 'filter
                in' or 'filter out'.</para>
              </listitem>
            </orderedlist></para>

          <para>As a result of these problems major changes are planed in post
          1.0.0 versions of Heritrix. The concept of scope will remain but how
          they are defined and configured will change drastically . Those
          changes will also greatly affect the filters (see <xref
          linkend="filters" />).</para>
        </sect4>
      </sect3>

      <sect3>
        <title id="processingchains">Processing Chains</title>

        <para>When a URI is crawled it is in fact passed through a series of
        processors. This series is split up into five chains and the user can
        add, remove and reorder the processors on each of these chains.</para>

        <para>While generally each processor only makes sense in one of these
        chains that is not currently enforced.</para>

        <para>Each URI taken off the Frontier queue runs through the
        <literal>Processing Chains</literal> listed in the diagram shown on
        the right. URIs are always processed in the order shown in the diagram
        unless a particular processor throws a fatal error. In this
        circumstance, processing skips to the end, to the Post-processing
        chain, for cleanup.</para>

        <para>Each processing chain is made up of zero or more individual
        processors. For example, the extractor processing chain might comprise
        the <literal>ExtractorHTML</literal> , an
        <literal>ExtractorJS</literal> , and the
        <literal>ExtractorUniversal</literal> processors. Within a processing
        step, the order in which processors are run is the order in which
        processors are listed in the job order file.</para>

        <para>Generally, particular processors only make sense with in the
        context of one particular processing chain. For example, it wouldn't
        make sense to run the <literal>FetchHTTP</literal> processor in the
        Post-processing chain.</para>

        <graphic fileref="processing_steps.png" format="PNG" />

        <para>Most of the processors are fairly self explanatory, however the
        first and last two merit a little bit more attention.</para>

        <para>In the <literal>Pre Processing</literal> chain the following two
        processors should be included (or replacement modules that perform
        similar operations:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Preselector</emphasis></para>

            <para>Last check if the URI should indeed be crawled. Can for
            example recheck scope. Useful if it has been changed after the
            crawl starts.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PreconditionEnforcer</emphasis></para>

            <para>Ensures that all preconditions for crawling a URI have been
            met. These currently include verifying that DNS and robots.txt
            information has been fetched for the URI.</para>
          </listitem>
        </itemizedlist>

        <para>Similarly the <literal>Post Processing</literal> chain has to
        special purpose processors:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">CrawlStateUpdater</emphasis></para>

            <para>Updates the per-host information that may have been affected
            by the fetch. This is currently robots and IP address info.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Postselector</emphasis></para>

            <para>Feeds discovered URIs back to the Frontier. Without this
            processor no discovered URI will ever be scheduled for
            crawling.</para>
          </listitem>
        </itemizedlist>

        <para>Within each chain the processors will be executed in the order
        in which they are displayed on the settings page.</para>
      </sect3>

      <sect3>
        <title>Statistics Tracking</title>

        <para>Any number of statistics tracking modules can be attached to a
        crawl. Currently only one is provided with Heritrix. The
        <literal>StatisticsTracker</literal> module that comes with Heritrix
        writes the progress-statistics.log file and provides the WUI with the
        data it needs to display progress information about a crawl. It is
        strongly recommended that any crawl running with the WUI use this
        module.</para>
      </sect3>
    </sect2>

    <sect2 id="filters">
      <title>Filters</title>

      <para>Filters are modules that take a <xref linkend="crawluri" /> and
      determines if it matches the criteria of the filter. If so it returns
      true, otherwise it returns false.</para>

      <para>Filters are used in a couple of different contexts in Heritrix.
      </para>

      <para>Their use in scopes has already been discussed in <xref
      linkend="scopes" /> and the problems with using them that in <xref
      linkend="scopeproblems" />. Along with the planned overhaul of the
      scopes (as discussed in <xref linkend="scopeproblems" />) the filters
      will likely change significantly.</para>

      <para>Aside from scopes filters are also used in processors. Filters
      applied to processors always filter URIs <emphasis>out</emphasis>. That
      is to say that any URI matching a filter on a processor will effectively
      skip over that processor. </para>

      <para>This can be useful to disable (for instance) link extraction on
      documents coming from a specific section of a given website.</para>

      <sect3>
        <title>Adding, removing and reordering filters</title>

        <para>The filters page of the configuration section of the WUI
        presents a semi treelike structure of the crawl configuration.
        Wherever filters can be added a listing of existing filters is
        provided along with the options to remove them or move them up or down
        that list. Below each list is the option to add a new filter to that
        list. </para>

        <para>Adding a new filters requires giving it a unique name, selecting
        the class type of the filter from a combobox and clicking the
        associated add button.</para>

        <para>Since filters can in turn contain other filters (the OrFilter
        being the best example of this) these lists can become quite complex
        and at times confusing.</para>
      </sect3>

      <sect3>
        <title>Provided filters</title>

        <para>The following is an overview of the most useful of the filters
        provided with Heritrix.</para>

        <sect4>
          <title>org.archive.crawler.filter.OrFilter</title>

          <para>Contains any number of filters and returns true if any of them
          returns true. A logical OR on it's filters basically.</para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.URIRegExpFilter</title>

          <para>Returns true if a URI matches the regular expression set for
          it.</para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.FilePatternFilter</title>

          <para>Compares suffix of a passed URI against a regular expression
          pattern, returns true for matches. </para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.PathDeptFilter</title>

          <para>Returns true for all <xref linkend="crawluri" /> passed in
          with a path depth less or equal than the
          <literal>max-path-depth</literal> value.</para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.PathologicalPathFilter</title>

          <para>Checks if a URI contains a repeated pattern.</para>

          <para> This filter is checking if a pattern is repeated a specific
          number of times. The use is to avoid crawler traps where the server
          adds the same pattern to the requested URI like:</para>

          <para><programlisting>  http://host/img/img/img/img....</programlisting></para>

          <para>Returns true if such a pattern is found. Sometimes used on
          processor but is primarily of use in the exclude section of
          scopes.</para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.HopsFilter</title>

          <para>Returns true for all URIs passed in with a <xref
          linkend="link-hop-count" /> greater than the
          <literal>max-link-hops</literal> value. </para>

          <para>Generally only used in scopes.</para>
        </sect4>

        <sect4>
          <title>org.archive.crawler.filter.TransclusionFilter</title>

          <para>Filter which returns true for <xref linkend="crawluri" />
          instances which contain more than zero but fewer than
          <literal>max-trans-hops</literal> embed entries at the end of their
          <xref linkend="discoverypath" />.</para>

          <para>Generally only used in scopes.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="credentials">
      <title>Credentials</title>

      <para>In this section you can add login credentials that will allow
      Heritrix to gain access to areas of websites requiring authentication.
      As with all modules they are only added here (supplying a unique name
      for each credential) and then configured on the settings page (<xref
      linkend="settings" />).</para>

      <para>One of the settings for each credential is it's
      <literal>credential-domain</literal> and thus it is possible to create
      all credentials on the global level. However since this can cause
      excessive unneeded checking of credentials it is recommended that
      credentials be added to the appropriate domain override (see <xref
      linkend="overrides" /> for details). That way the credential is only
      checked when the relevant domain is being crawled.</para>

      <para>Heritrix can do two types of authentication: <ulink
      url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC and
      DIGEST Auth) and POST and GET of an HTML Form.</para>

      <note>
        <title>Logging</title>

        <para>To enable logging of authentication interactions, set the
        FetchHTTP log level to fine</para>

        <para><programlisting>org.archive.crawler.fetcher.FetchHTTP.level = FINE</programlisting></para>

        <para>This is done by editing the
        <filename>heritrix.properties</filename> file under the
        <filename>conf</filename> directory.</para>
      </note>

      <sect3>
        <title><ulink
        url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC and
        DIGEST Auth)</title>

        <para>Supply <ulink url="#cd">Credential Domain</ulink>, <ulink
        url="realm">Realm</ulink>, login, and password.</para>

        <para>The way that the RFC2617 authentication works in Heritrix is
        that in response to a 401 response code (Unauthorized), Heritrix will
        use a key made up of the Credential Domain plus Realm to do a lookup
        into its Credential Store. If a match is found, then the credential is
        loaded into the CrawlURI and the CrawlURI is marked for immediate
        retry.</para>

        <para>When the requeued CrawlURI comes around again, this time
        through, the found credentials are added to the request. If the
        request succeeds -- result code of 200 -- the credentials are promoted
        to the CrawlServer and all subsequent requests made against this
        CrawlServer will preemptively volunteer the credential. If the
        credential fails -- we get another 401 -- then the URI is let die a
        natural 401 death.</para>

        <sect4 id="cd">
          <title>Credential Domain</title>

          <para>This equates to the canonical root URI of RFC2617;
          effectively, in our case, its the CrawlServer name or <ulink
          url="http://java.sun.com/j2se/1.4.2/docs/api/java/net/URI.html">URI
          authority</ulink> (domain plus port if other than port 80).</para>
        </sect4>

        <sect4 id="realm">
          <title>Realm</title>

          <para>Realm as per <ulink
          url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink>. The
          realm string must match exactly the realm name presented in the
          authentication challenge served up by the web server</para>
        </sect4>

        <sect4>
          <title>Known Limitations</title>

          <sect5>
            <title>One Realm per Credential Domain Only</title>

            <para>Currently, you can only have one realm per credential
            domain.</para>
          </sect5>

          <sect5>
            <title>Digest Auth works for Apache</title>

            <para>... but your mileage may vary going up against other servers
            (See <ulink
            url="http://sourceforge.net/tracker/index.php?func=detail&amp;aid=914301&amp;group_id=73833&amp;atid=539102">[
            914301 ] Logging in (HTTP POST, Basic Auth, etc.)</ulink> to learn
            more).</para>
          </sect5>
        </sect4>
      </sect3>

      <sect3>
        <title>HTML Form POST or GET</title>

        <para></para>
      </sect3>
    </sect2>

    <sect2 id="settings">
      <title>Settings</title>

      <para>This page presents a semi treelike representation of all the
      modules (fixed and pluggable alike) that make up the current
      configuration and allows the user to edit any of their settings.</para>

      <para>The first option presented directly under the top tabs is whether
      or to hide or display 'expert settings'. Expert settings are those
      settings that are rarely changed and should only be changed by someone
      with a clear understanding of their implication. This document will not
      discuss any of the expert settings.</para>

      <para>The first setting is the description of the job previously
      discussed. The seed list is at the bottom of the page. Between the two
      are all the other possible settings.</para>

      <para>Module names are presented in bold and a short explanation of them
      is provided. As discussed in the previous three chapters some of them
      can be replaced, removed or augmented.</para>

      <para>Behind each module and settings name a small question mark is
      present. By clicking on it a more detailed explanation of the relevant
      item pops up. For most settings users should refer to that as their
      primary source of information.</para>

      <para>Some settings provide a fixed number of possible 'legal' values in
      combo boxes. Most are however typical text input fields. Two types of
      settings require a bit of additional attention.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Lists</emphasis></para>

          <para>Some settings are a list of values. In those cases a list is
          printed with an associated remove button and an input box is printed
          below it with an add button. Only those items in the list box are
          considered in the list itself. A value in the input box does not
          become a part of the list until the user clicks add. There is no way
          to edit existing values beyond removing them and replacing them with
          correct values. It is also no possible to reorder the list.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Simple typed maps</emphasis></para>

          <para>Generally Maps in the Heritrix settings framework contain
          program modules (such as the processors for example) and are
          therefor edited elsewhere. However maps that only accept simple data
          types (Java primitives) can be edited here.</para>

          <para>They are treated as a key, value pair. Two input boxes are
          provided for new entries with the first one representing the key and
          the second the value. Clicking the associated add button adds the
          entry to the map. Above the input boxes a list of existing entries
          is displayed along with a remove option. Simple maps can not be
          reordered.</para>
        </listitem>
      </itemizedlist>

      <para>Changes on this page are not saved until you navigate to another
      part of the settings framework or you click the submit job/finished
      tab.</para>

      <para>If there is a problem with one of the settings a red star will
      appear next to it. Clicking the star will display the relevant error
      message.</para>
    </sect2>

    <sect2 id="overrides">
      <title>Overrides</title>

      <para>Overrides provide the ability to override individual settings on a
      per domain bases. The overrides page provides an iterative list of
      domains that contain override settings, that is values for parameters
      that override values in the global configuration.</para>

      <para>It is best to think of the general global settings as the root of
      the settings hierarchy and they are then overridden by top level domains
      (com, net, org, etc) who are in turn overridden by domains (yahoo.com,
      archive.org, etc.) who can further be overridden by sub domains
      (crawler.archive.org). There is no limit for how deep into the sub
      domains the overrides can go.</para>

      <para>When a URI is being processed the settings for it's host is then
      first lookup up. If the needed setting is not available there it's super
      domains are checked until the settings is found (all settings exist at
      the global level at the very least).</para>

      <para>Creating a new override is done by simply typing in the domain in
      the input box at the bottom of the page and clicking the 'Create/Edit'
      button. Alternatively if overrides already exist the user can navigate
      the hierarchy of existing overrides, edit them and create new overrides
      on domains that don't already have them.</para>

      <para>Once an override has been created or selected for editing the user
      is taken to a page that closely resembles the settings page discussed in
      <xref linkend="settings" />. The main difference is that those settings
      that can not be overridden (file locations, number of threads etc.) are
      printed in a non-editable manner. Those settings that can be edited now
      have a checkbox in front of them. If they are being overridden at the
      current level that checkbox should be checked. Editing a setting will
      cause the checkmark to appear. Removing the checkmark effectively
      removes the override on that setting.</para>

      <para>Once on the settings page the second level tabs will change to
      override context. The new tabs will be similar to the general tabs and
      will have:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Filters</emphasis></para>

          <para>Add filters to the override. It is not possible to remove
          inherited filters or interject new filters among them. New filters
          will be added after existing filters. All filters though have to
          option to disable them that can be set on the settings page.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Credentials</emphasis></para>

          <para>Add credentials to the override. Generally credentials should
          always be added to an override of the domain most relevant to them.
          See <xref linkend="credentials" /> for more details.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Settings</emphasis></para>

          <para>Page allowing the user to override specific settings as
          discussed above.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Refinements</emphasis></para>

          <para>Manage refinements for the override. See <xref
          linkend="refinements" /></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Done with override</emphasis></para>

          <para>Once the user has finished with the override, this option will
          take him back to the overrides overview page.</para>
        </listitem>
      </itemizedlist>

      <para>It is not possible to add, remove or reorder existing modules on
      an override. It is only possible to add filters and credentials. Those
      added will be inherited to sub domains of the current override domain.
      Those modules that are added in an override will not have a checkbox in
      front of their settings on the override settings page since the override
      is effectively their 'root' domain.</para>

      <para>Finally, due to how the settings framework is structured there is
      negligible performance penalty to using overrides. Lookups for settings
      take as much time whether or not overrides have been defined. For URIs
      belonging to domains without overrides no performance penalty is
      incurred.</para>
    </sect2>

    <sect2 id="refinements">
      <title>Refinements</title>

      <para>Refinements are similar to overrides (see <xref
      linkend="overrides" />) in that the allow the user to modify the
      settings under certain circumstances. There are however two major
      differences.</para>

      <orderedlist>
        <listitem>
          <para>Refinements are applied based on arbitrary criteria rather
          then encountered URIs domain.</para>

          <para>Currently it is possible to set criteria based on the time of
          day, a regular expression matching the URI and the port number of
          the URI.</para>
        </listitem>

        <listitem>
          <para>They incur a performance penalty.</para>

          <para>This effect is small if there numbers are few but for each URI
          encountered there must be a check made to see if it matches any of
          the existing criteria of defined refinements.</para>

          <para>This effect can be mitigated by applying refinements to
          overrides rather then the global settings.</para>
        </listitem>
      </orderedlist>

      <para>Refinements can be applied either to the global settings or to any
      override. If applied to an override they can affect any settings,
      regardless of whether the parent override has modified it.</para>

      <para>It is not possible to create refinements on refinements.</para>

      <para>Clicking the refinements tab on either the global settings or an
      override brings the user to the refinements overview page. The overview
      page displays a list of existing refinements on the current level and
      allows the user to create new ones.</para>

      <para>To create a new override the user must supply a unique name for it
      (name is limited to letters, numbers, dash and underscore) and a short
      description that will be displayed underneath it on the overview
      page.</para>

      <para>Once created, refinements can be either removed or edited.</para>

      <para>Choosing the edit option on an override brings the user to the
      criteria page. Aside from the criteria tab replacing the refinements
      tab, the second level tabs will have the same options as they do for
      overrides and their behavior will be the same. Clicking the 'Done with
      refinement' tab will bring the user back to the refinements overview
      page.</para>

      <sect3>
        <title>Criteria</title>

        <para>The criteria page displays a list of the current criteria and
        the option to add any of the available criteria types to the list. It
        is also possible to remove existing criteria.</para>

        <note>
          <para>URIs must match <emphasis role="bold">all</emphasis> set
          criteria for the refinement to take effect.</para>
        </note>

        <para>Currently the following criteria can be applied:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Port number</emphasis></para>

            <para>Match only those URIs for the given port number.</para>

            <para>Default port number for HTTP is 80 and for 443 HTTPS.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Time of day</emphasis></para>

            <para>If this criteria is applied the refinement will be in effect
            between the hours specified each day.</para>

            <para>The format for the input boxes is HHMM (hours and
            minutes).</para>

            <para>An example might be: From 0200, To 0600. This refinement
            would be in effect between 2 and 6 am each night. Possibly to ease
            the politeness requirements during these hours when load on
            websites is generally low.<note>
                <para>As with all times in Heritrix these are <emphasis
                role="bold">always GMT</emphasis> times.</para>
              </note></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Regular expression</emphasis></para>

            <para>The refinement will only be in effect for those URIs that
            match the given regular expression.<note>
                <para>See <xref linkend="regexpr" /> for more on them.</para>
              </note></para>
          </listitem>
        </itemizedlist>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="running">
    <title>Running a job</title>

    <para>Once a crawl job has been created and properly configured it can be
    run. To start a crawl the user must go to the Console page (via the
    Console tab).</para>

    <sect2 id="console">
      <title>Console</title>

      <para>The Console presents on overview of the current status of the
      crawler.</para>

      <sect3>
        <title>Crawler status</title>

        <para>Regardless of if there is job being crawled the following
        information is provided:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Crawler running</emphasis></para>

            <para>Is the crawler running? If yes then the next pending job
            will be crawled as soon as the current one is completed. For more
            detail see <xref linkend="runningvcrawling" />.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Current job</emphasis></para>

            <para>The name of the job currently being crawled. If no job is
            being crawled this will read <emphasis>none</emphasis>.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Jobs pending</emphasis></para>

            <para>The number of jobs pending. That is waiting for their turn
            to be crawled.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Jobs completed</emphasis></para>

            <para>The number of jobs that have been crawled (including those
            that failed to start for some reason (see <xref
            linkend="failedtostart" /> for more on misconfigured jobs).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Used memory</emphasis></para>

            <para>The amount of memory (in kilobytes) currently being used by
            the program.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Heap size</emphasis></para>

            <para>The amount of memory (in kilobytes) that the JavaVM has
            allocated itself.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Max heap size</emphasis></para>

            <para>The amount of memory (in kilobytes) that the JavaVM can
            <emphasis>at most </emphasis>allocate itself.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Alerts</emphasis></para>

            <para>Total number of alerts, and within brackets new alerts, if
            any.</para>

            <para>See <xref linkend="alerts" /> for more on alerts.</para>
          </listitem>
        </itemizedlist>

        <para>If a job is being crawled the following information is also
        provided:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Status</emphasis></para>

            <para>The current status of the job in progress. Jobs being
            crawled are either running, paused or waiting to pause.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Processed docs/sec</emphasis></para>

            <para>There are two values. The first is the number of documents
            downloaded per second between the last snapshot of the Frontier's
            state and the one before that (typically about 10-20 seconds). It
            gives some indication of the current rate of progress but can
            fluctuate considerable, especially if the StatisticsTracker has
            the log interval set to a low value. </para>

            <para>The second value (in parenthasis) is the number of documents
            downloaded per second since the crawl began.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Processed KB/sec</emphasis></para>

            <para>These are two values comparable with the docs/sec ones. Only
            these indicate the number of KB (see <xref linkend="bytes" />)
            downloaded per second rather then documents.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Run time</emphasis></para>

            <para>The amount of time that has elapsed since the crawl began
            (excluding any time spent paused).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Active thread count</emphasis></para>

            <para>The number of threads that are processing a URI of the total
            number of threads that have been created.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Total data written</emphasis></para>

            <para>The amount of data downloaded since the crawl began. See
            <xref linkend="bytes" />.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Downloaded/Queued document
            ratio</emphasis></para>

            <para>This is the ratio between successfully downloaded (i.e. got
            a response from the web server, may have been a 404 etc.) and the
            total number of queued (i.e. waiting to be crawled) and
            successfully downloaded URIs.</para>

            <para>For broad crawls this will likely never increase, may in
            fact decrease and is likely to stay at the 5-6% range.</para>

            <para>For more limited crawls (domain or host scoped) this will
            eventually reach 100% once the relevant scope is exhausted.</para>
          </listitem>
        </itemizedlist>

        <para>Some of this information is replicated in the head of each page
        (see <xref linkend="header" />).</para>
      </sect3>

      <sect3>
        <title>Options</title>

        <para>Below the status display is a series of options. The exact
        options available depend on whether the crawler is running or not, and
        if so if there is a job being crawled and whether or not that job is
        paused or not.</para>

        <sect4>
          <title>Start / Stop crawling pending job</title>

          <para>If the crawler is running the option to start it is afforded.
          If it is already running the option to stop is provided.</para>

          <para>If the crawler is stopped, a job currently running will
          continue to run until it finishes. Stopping the crawler thus only
          prevents the next job from being run, it does not affect the current
          job. A separate option is provided for terminating the current
          job.<note>
              <para>See <xref linkend="runningvcrawling" /> for more on those
              terms.</para>
            </note></para>
        </sect4>

        <sect4>
          <title>Terminate current job</title>

          <para>Terminates the current job. The job's status will be marked as
          'Aborted by user'.</para>

          <para>All currently active threads will be allowed to finish behind
          the scenes even though the WUI will report the job being terminated
          at once.</para>

          <para>The next job (if one is pending) will start immediately
          despite this.</para>
        </sect4>

        <sect4>
          <title>Pause / Resume current job</title>

          <para>Issue a pause or resume (if already paused) order to the crawl
          in progress. Once the order to pause is issued it may take some time
          for all the active threads to enter a paused state. Until then the
          job is considered to be still running and 'waiting to pause'. It is
          possible to resume from that waiting state.</para>

          <para>Once paused the job is considered to be suspended and time
          spent in that state does not count towards the total time spent
          crawling that is used to calculate document per second rates
          etc.</para>
        </sect4>

        <sect4>
          <title>Inspect frontier URIs</title>

          <para>Only available for paused jobs (and only for those that
          <emphasis>have</emphasis> paused, not those that have been told to
          pause and are waiting for threads tor finish).</para>

          <para>Takes the user to a page that allows him to lookup URIs in the
          frontier and to delete them by using a regular expression.</para>
        </sect4>

        <sect4>
          <title>Refresh</title>

          <para>Update the status display. The status display does not update
          itself and quickly becomes out of date as crawling proceeds. This
          also refreshes the options available if they've changed as a result
          of a change in the state of the job being crawled.</para>
        </sect4>

        <sect4>
          <title>Shut down Heritrix</title>

          <para>It is possible to shut down Heritrix through this option.
          Doing so will terminate the Java process running Heritrix and the
          only way to start it up again will be via command line as this also
          disables the WUI.</para>

          <para>The user is asked to confirm this action twice to prevent
          accidental shut downs.</para>

          <para>This option will try to terminate any current job gracefully
          but will only wait a very short time for active threads to
          finish.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2 id="pendingjobs">
      <title>Pending jobs</title>

      <para>At any given time there can be any number of crawl jobs waiting
      for their turn to be crawled. </para>

      <para>From the Jobs tab the user can access a list these pending jobs
      (it also possible to get to them from the header, see <xref
      linkend="header" />).</para>

      <para>The list displays the name of each job, it's status (currently all
      pending jobs have the status 'Pending') and offers the following options
      for each job:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">View order</emphasis></para>

          <para>Opens up the actual XML configuration file in a seperate
          window. Of interest to advanced users only.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Edit configuration</emphasis></para>

          <para>Takes the user to the Settings page of the jobs configurations
          (see <xref linkend="settings" />).</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Journal</emphasis></para>

          <para>Takes the user to the job's Journal (see <xref
          linkend="journal" />).</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Delete</emphasis></para>

          <para>Deletes the job (will only be marked as deleted, does not
          delete it from disk).</para>
        </listitem>
      </itemizedlist>
    </sect2>

    <sect2>
      <title>Monitoring a running job</title>

      <para></para>

      <sect3>
        <title>Internal reports on ongoing crawl</title>

        <para>The following reports are only availible while the crawler is
        running. They provide information about the internal status of certain
        parts of the crawler. Generally this information is only of interest
        to advanced users who possess detailed knowledge of the internal
        workings of said modules.</para>

        <para>These reports can be accessed from the Reports tab when a job is
        being crawled.</para>

        <sect4>
          <title>Frontier report</title>

          <para>A report on the internal state of the frontier.</para>
        </sect4>

        <sect4>
          <title>Thread report</title>

          <para>Contains information about what each toe thread is doing and
          how long it's been doing it. Also allows users to terminate threads
          that have become stuck. Terminated threads will not actually be
          removed from memory, Java does not provide a way of doing that.
          Instead they will be isolated from the rest of the program running
          and the URI they are working on will be reported back to the
          frontier as if it had failed to be processed.<caution>
              <para>Terminating threads should only be done by advanced users
              who understand the effect of doing so.</para>
            </caution></para>
        </sect4>

        <sect4 id="processorsreport">
          <title>Processors report</title>

          <para>A report on each processor. Not all processors provide
          reports. Typically these are numbers of URIs handled, links
          extracted etc.</para>

          <para>This report is saved to a file at the end of the crawl (see
          <xref linkend="processorsreport.txt" />).</para>
        </sect4>
      </sect3>

      <sect3 id="failedtostart">
        <title>Job failed to start</title>

        <para>If a job is misconfigured in such a way that it is not possible
        to do any crawling it might seem as if it never started. In fact what
        happens is that the crawl is started but on the initialization it is
        immediately terminated and sent to the list of completed jobs (<xref
        linkend="completedjobs" />). In those instances an explanation of what
        went wrong is displayed on the completed jobs page. An alert will also
        be created.</para>

        <para>A common cause of this is forgetting to set the HTTP header's
        <literal>user-agent</literal> and <literal>from</literal> attributes
        to valid values. The <literal>from</literal> attribute must contain a
        valid e-mail address and the <literal>user-agent</literal> most be in
        the following form:</para>

        <para><programlisting>  [name] (+[http-url])[optional-etc]</programlisting></para>

        <para>If no processors are set on the job (or the modules otherwise
        badly misconfigured) the job may succeed in initializing but
        immediately exhaust the seed list, failing to actually download
        anything. This will not trigger any errors but a review of the logs
        for the job should highlight the problem. So if a job terminates
        immediately after starting without errors, the configuration
        (especially modules) should be reviewed for errors.</para>
      </sect3>

      <sect3 id="header">
        <title>All page status header</title>

        <para></para>
      </sect3>

      <sect3>
        <title id="alerts">Alerts</title>

        <para>The number of existing and new alerts is displayed both in the
        Console (<xref linkend="console" />) and the header of each page
        (<xref linkend="header" />).</para>

        <para>Clicking on the link made up of those numbers takes the user to
        an overview of the alerts. The alerts are presented as messages, with
        unread ones clearly marked in bold and offering the user the option of
        reading them, marking as read and deleting them.</para>

        <para>Clicking an alert brings up a screen with it's details.</para>

        <para>Alerts are generated in response to an error or problem of some
        form. Alerts have severity levels that mirror the Java log levels.
        </para>

        <para>Serious exception that occur will have a
        <emphasis>Severe</emphasis> level. These may be indicative of bugs in
        the code or problems with the configuration of a crawl job.</para>
      </sect3>
    </sect2>

    <sect2>
      <title>Editing a running job</title>

      <para>The configurations of a job can be edited while it is running.
      This option is accessed from the Jobs tab (Current job/Edit
      configuration). When selected the user is taken to the settings section
      of the job's configuration (<xref linkend="settings" />).</para>

      <para>When a configuration file is edited, the old version of it is
      saved to a new file (new file is named
      <filename>&lt;oldFilename&gt;_&lt;timestamp&gt;.xml</filename>) before
      it is updated. This way a record is kept of any changes. This record is
      only kept for changes made <emphasis>after</emphasis> crawling
      begins.</para>

      <para>It is not possible to edit all aspects of the configuration after
      crawling starts. Most noticably the Modules section is disabled. Also,
      although not enforced by the WUI, making changes to certain settings (in
      particular filenames, directory locations etc.) will have no effect
      (doing so is will not harm the crawl, it will simply be ignored).</para>

      <para>However most settings can be changed. This includes the number of
      threads being used and the seeds list and although it is not possible to
      remove modules, most have the option to disable them. Settings a modules
      <literal>enabled</literal> attribute to <literal>false</literal>
      effectively removes them from the configuration.</para>

      <sect3>
        <title id="journal">Journal</title>

        <para>The user can add notes to a journal that is kept for each job.
        No entries are made automatically in the journal, it is only for user
        added comments.</para>

        <para>It can be useful to use it to document reasons behind
        configuration changes to preserve that information along with the
        actual changes.</para>

        <para>The journal can be accessed from the Pending jobs page (<xref
        linkend="pendingjobs" />) for pending jobs, the Jobs tab for currently
        running jobs and the Completed jobs page (<xref
        linkend="completedjobs" />) for completed jobs.</para>
      </sect3>
    </sect2>
  </sect1>

  <sect1>
    <title>Analysis of jobs</title>

    <para></para>

    <sect2 id="completedjobs">
      <title>Completed jobs</title>

      <para></para>
    </sect2>

    <sect2 id="logs">
      <title>Logs</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Reports</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Outside the user interface</title>

      <para></para>

      <sect3>
        <title>Generated files</title>

        <para>In addition to the logs discussed before (see <xref
        linkend="logs" />) the following files are generated.</para>

        <sect4>
          <title>heritrix_out.log</title>

          <para></para>
        </sect4>

        <sect4>
          <title>crawl-manifest.txt</title>

          <para></para>
        </sect4>

        <sect4>
          <title>crawl-report.txt</title>

          <para></para>
        </sect4>

        <sect4>
          <title>hosts-report.txt</title>

          <para></para>
        </sect4>

        <sect4>
          <title>mimetype-report.txt</title>

          <para></para>
        </sect4>

        <sect4 id="processorsreport.txt">
          <title>processors-report.txt</title>

          <para>Contains the processors report (see <xref
          linkend="processorsreport" />) generated at the very end of the
          crawl.</para>
        </sect4>

        <sect4>
          <title>responsecode-report.txt</title>

          <para></para>
        </sect4>

        <sect4>
          <title>seeds-report.txt</title>

          <para></para>
        </sect4>

        <sect4>
          <title>ARC files</title>

          <para>Assuming that you are using the ARC writer that comes with
          Heritrix a number of ARC files will be generated containing the
          crawled pages.</para>

          <para>It is possible to specify the location of these files on the
          ARCWriter processor in settings page.</para>

          <para>ARC files are named as follows:</para>

          <para><programlisting>  [prefix]-[series#-padded-to-5-digits]-[12-digit-timestamp]-[Base32-of-crawler-IP].arc.gz</programlisting></para>

          <para>The <literal>prefix</literal> is set by the user when he
          configures the ARCWriter processor. By default it is IAH.</para>
        </sect4>
      </sect3>
    </sect2>
  </sect1>

  <glossary>
    <glossdiv>
      <title>Some definitions</title>

      <glossentry id="runningvcrawling">
        <glossterm>Running Vs. Crawling</glossterm>

        <glossdef>
          <para>The state <emphasis>running</emphasis> generally means that
          the crawler will start executing a job as soon as one is made
          available in the pending jobs queue (as long as there is not a job
          currently being run).</para>

          <para>If the crawler is not in the running state, jobs added to the
          pending jobs queue will be held there in stasis; they will not be
          run, even if there are no jobs currently being run.</para>

          <para>The term <emphasis>crawling</emphasis> generally refers to a
          state whereby a job's being currently run (crawled): i.e. pages are
          being fetched, links extracted etc.</para>
        </glossdef>
      </glossentry>

      <glossentry id="regexpr">
        <glossterm>Regular expressions</glossterm>

        <glossdef>
          <para>All regular expressions used by Heritrix are Java regular
          expressions.</para>

          <para>Java regular expressions differ from those used in Perl, for
          example, in several ways. For detailed info on Java regular
          expressions see the Java API for java.util.regex.Pattern on Sun's
          home page (<ulink
          url="http://java.sun.com">java.sun.com</ulink>).</para>

          <para>For API of Java SE v1.4.2 see <ulink
          url="http://java.sun.com/j2se/1.4.2/docs/api/index.html">http://java.sun.com/j2se/1.4.2/docs/api/index.html</ulink>,
          it is recommended you lookup the API for the version of Java that is
          being used to run Heritrix.</para>
        </glossdef>
      </glossentry>

      <glossentry id="dates">
        <glossterm>Dates and times</glossterm>

        <glossdef>
          <para>All times in Heritrix are GMT assuming the clock on the
          machine Heritrix is running on is correct.</para>

          <para>This means that all date/time stamps in logs are GMT, all
          dates and times shown in the WUI are GMT and any times or dates
          entered by the user need to be in GMT.</para>
        </glossdef>
      </glossentry>

      <glossentry id="bytes">
        <glossterm>Bytes, KB and statistics</glossterm>

        <glossdef>
          <para>Heritrix adheres to the following conventions for displaying
          byte and bit amounts:</para>

          <para><programlisting>  Legend Type
       B Bytes
      KB Kilobytes - 1 KB = 1024 B
      MB Megabytes - 1 MB = 1024 KB
      GB Gigabytes - 1 GB = 1024 MB
  
       b bits
      Kb Kilobits - 1 Kb = 1000 b
      Mb Megabits - 1 Mb = 1000 Kb
      Gb Gigabits - 1 Gb = 1000 Mb</programlisting></para>

          <para>This also applies to all logs.</para>
        </glossdef>
      </glossentry>

      <glossentry id="crawluri">
        <glossterm>CrawlURI</glossterm>

        <glossdef>
          <para>A URI and it's associated data such as parent URI, number of
          links from seed etc.</para>
        </glossdef>
      </glossentry>

      <glossentry id="link-hop-count">
        <glossterm>Link hop count</glossterm>

        <glossdef>
          <para>Number of link follow from the seed to the current URI. Seeds
          have a link hop count of 0.</para>
        </glossdef>
      </glossentry>

      <glossentry>
        <glossterm>Discovery path</glossterm>

        <glossdef>
          <para>Each URI has a discovery path. The path contains one character
          for each link or embed followed from the seed.</para>

          <para>The character legend is as follows.</para>

          <programlisting>  R - Redirect
  E - Embed
  X - Speculative embed (aggressive link extraction)
  L - Link</programlisting>
        </glossdef>
      </glossentry>
    </glossdiv>
  </glossary>
</article>