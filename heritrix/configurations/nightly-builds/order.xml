<!-- This is a generic configuration file that can be used to start a Heritrix crawl -->
<crawl-order name="GardenCrawl">
 <comment>
  For information on the Heritrix open source crawler project please see http://crawler.arcive.org/.
 </comment>
 
<!-- The following elements determinethe directory to which ouptut files (arcs, logs, etc)
 		are written, and the prefix given to arc files (disk and arc-file elements, respectively).
-->
  <disk path="arcs/" />
 <arc-file prefix="NIGHTLY_BUILD-" />
 
 
<!-- crawler-behavior defines the way in which the crawler will behave (as you would guess).  
		Specifically, it should be used  to specify things like what pages get processed, how the
		are processed, how often pages are re-fetched, what gets written to disk, etc.
-->
<crawler-behavior>

<!-- 	The following headers will be sent to servers when requesting content.  They should be
		set so that site maintainers can get in touch with you in the event that your crawl
		causes problems.  The crawler will not run if you leave this value as the default.
-->
 <http-headers>
  <User-Agent>Heritrix pre-Alpha Nightly Build Crawl (contact gojomo@archive.org)</User-Agent>
  <From>crawler-devel@archive.org</From>
 </http-headers>
 
<!-- 	Following is a list of artifical limits that may be placed on the crawler.
  		These limits can, in some cases, be overridden in the configuration blocks
  		that apply to specific processors (see below)
  		DEFINE EACH OF THESE VALUES
-->
 <limits>
  <max-pages value="1000" />
  <max-duration value="1h" />
  <max-resources-per-site value="1000" />
  <max-toe-threads value="1" />
  <max-link-depth value="100" />
 </limits>
 
  
 <!-- The folloing values elements (schedule, store, and selector) represent
 		the fundemental crawler components and should probably not be 
 		changed
 -->
 <scheduler class="org.archive.crawler.basic.SimpleScheduler" />
 <store class="org.archive.crawler.basic.SimpleStore" />
 <selector class="org.archive.crawler.basic.SimpleSelector">
  <seeds src="seeds.txt" />
 </selector>
 
 
 <!-- Following is a list of processors, or modules, which will process serially each
 		fetched resource (URI).  Processors may be simple declarations and have only
 		one behavior, or may be configurable.  Configurable processors will be documented
 		below.
-->
 <processors>
 
<!-- The precondition enforcer makes sure URIs are not processed before
  			prerequsities.  For example, a sites' robots.txt must be fetched before
  			any other pages, and a dns lookup must occur before even this is done.
  			
			This module also enforces politeness, which is configurable by setting
			the delay-factor param.  DEFINE DELAY FACTOR HERE.
-->
  <processor 
    name="Preprocessor" 
    class="org.archive.crawler.basic.SimplePreconditionEnforcer"
    next="DNS">
   <params delay-factor="1" />
  </processor>
  
<!-- The DNS fetcher module processes only DNS URIs, fetching DNS
  		information and storing it to the host cache 
-->
  <processor 
    name="DNS" 
    class="org.archive.crawler.basic.FetcherDNS"
    next="HTTP">
  </processor>
  
<!-- The following modules, Extractor* extract URIs from various
  		types of documents.  These cannot currently be configured, 
  		though you may remove modules that don't apply to your crawl.
-->  
  <processor 
    name="HTTP" 
    class="org.archive.crawler.basic.FetcherHTTPSimple"
    next="ExtractorHTTP">
  </processor>
  <processor 
    name="ExtractorHTTP" 
    class="org.archive.crawler.extractor.ExtractorHTTP"
    next="ExtractorHTML">
  </processor>
  <processor 
    name="ExtractorHTML" 
    class="org.archive.crawler.extractor.ExtractorHTML"
     next="ExtractorDOC">
  </processor>
  <processor
  	name="ExtractorDOC"
  	class="org.archive.crawler.extractor.ExtractorDOC"
  	next="ExtractorSWF">
  </processor>
  <processor
    name="ExtractorSWF"
    class="org.archive.crawler.extractor.ExtractorSWF"
    next="ExtractorPDF"
    >
 </processor>
 <processor
   	name="ExtractorPDF"
   	class="org.archive.crawler.extractor.ExtractorPDF"
    next="Archiver">
  </processor>
  
<!-- The ArcWriter module writes successfully fetched URIs
 		out to disk in standard archive format.  You may configure
 		this module in several ways.  You may write compressed
 		or uncompressed files using by modifying the 
 		compression element's use attribute (defaults to true).
 		You may configure the output file size, which is set as part
 		of the arc-files element, and is in bytes.
 		
 		You may also set up output filters, so that URIs are 
 		omitted (despite processing) based on regular expressons.
 		By default all fetched URIs will be written, however there is 
 		a commented out filter declarion below that demonstrates
 		how to use a filter to exclude files (in this case all non-http)
 		URIs. 
-->
  <processor 
    name="Archiver" 
    class="org.archive.crawler.basic.ARCWriter"
    next="Cacher">
    	<compression use="true"/>
    	<arc-files max-size-bytes="100000000"/>
    	<!-- 
    	<filter 
    	 name="http-only"
    	 class="org.archive.crawler.util.URIRegExpFilter"
    	 regexp="^http://" />
    	 -->
  </processor>
  
<!-- The CrawlStateUpdater is responsible for evaluating URIs after
		other processing stages and making any changes that need be
		made to the crawl at large based on the state of the URI.  It
		cannot be configured and should generally not need to be considered
-->
  <processor 
    name="Cacher" 
    class="org.archive.crawler.basic.CrawlStateUpdater">
  </processor>
 </processors>
 
</crawler-behavior>

<!-- The following element block allows you to configure the ways in
		which various loggers behave.
-->
<loggers>
  <crawl-statistics>
    <interval>60</interval>
  </crawl-statistics>
</loggers>

</crawl-order>