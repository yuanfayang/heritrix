<crawler-behavior>

 <http-headers>
  <User-Agent>Heritrix pre-Alpha (new ia-archiver; problems to gojomo@archive.org)</User-Agent>
  <From>gojomo@archive.org</From>
 </http-headers>
 
 <scheduler class="org.archive.crawler.basic.SimpleScheduler" />
 <store class="org.archive.crawler.basic.SimpleStore" />
 <selector class="org.archive.crawler.basic.SimpleSelector">
  <filter 
   name="pathological-path" 
   class="org.archive.crawler.util.URIRegExpFilter"
   modifier="not"
   regexp=".*/(.*)/\1/\1/.*" />
 </selector>
 
 <processors>
  <processor 
    name="Preprocessor" 
    class="org.archive.crawler.basic.SimplePreconditionEnforcer"
    next="DNS">
   <params delay-factor="20" />
  </processor>
  <processor 
    name="DNS" 
    class="org.archive.crawler.basic.SimpleDNSFetcher"
    next="HTTP">
  </processor>
  <processor 
    name="HTTP" 
    class="org.archive.crawler.basic.SimpleHTTPFetcher"
    next="Extractor">
    <params timeout-seconds="10" />
  </processor>
   <processor 
    name="Extractor" 
    class="org.archive.crawler.basic.SimpleLinkExtractor"
    next="Archiver">
  </processor>
  <processor 
    name="Archiver" 
    class="org.archive.crawler.basic.ARCWriter"
    next="Updater">
     <compression use="true"/>
	 <arc-files max-size-bytes="20000000"/>
	 <filter 
	  name="http-only"
	  class="org.archive.crawler.util.URIRegExpFilter"
	  regexp="^http://.*" />
  </processor>
  <processor 
    name="Updater" 
    class="org.archive.crawler.basic.HostInfoUpdater">
  </processor>
 </processors>
 
 <limits>
  <!-- actual enforcement of these limits may depend on choice 
       of SSS/processor instances that read and respect these limits -->
  <max-link-depth value="10" /> <!-- zero means crawl seeds only -->
  <max-pages value="1000" />
  <max-duration value="1h" />
  <max-resources-per-site value="1000" />
  <max-toe-threads value="5" />
 </limits>

</crawler-behavior>