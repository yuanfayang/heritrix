<crawl-order name="example-crawl">
 <comment>
  A simple crawl for example purposes.
 </comment>
 
 <crawler-behavior>

  <http-headers>
   <User-Agent>ArchiveOpenCrawler Alpha (new ia-archiver; problems to gojomo@archive.org)</User-Agent>
   <From>gojomo@archive.org</From>
  </http-headers>
 
  <selector class="org.archive.crawler.basic.SimpleSelector">
   <seeds src="example-seeds.txt" />
   <filter 
    name="pathological-path" 
    class="org.archive.crawler.util.URIRegExpFilter"
    modifier="not"
    regexp=".*/(.*)/\1/\1/.*" />
   <filter 
    name="no-more-than-20-slashes" 
    class="org.archive.crawler.util.URIRegExpFilter"
    modifier="not"
    regexp=".*//.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*/.*" />
  </selector>
  
  <scheduler class="org.archive.crawler.basic.SimpleScheduler" />
  
  <store class="org.archive.crawler.basic.SimpleStore" />
 
  <processors>
   <processor 
     name="Preprocessor" 
     class="org.archive.crawler.basic.SimplePreconditionEnforcer"
     next="DNS">
    <params delay-factor="5" />
    <params minimum-delay="100" />
   </processor>
   <processor 
     name="DNS" 
     class="org.archive.crawler.basic.FetcherDNS"
     next="HTTP">
   </processor>
   <processor 
     name="HTTP" 
     class="org.archive.crawler.basic.FetcherHTTPSimple"
     next="ExtractorHTTP">
     <params timeout-seconds="10" />
   </processor>
   <processor 
     name="ExtractorHTTP" 
     class="org.archive.crawler.extractor.ExtractorHTTP"
     next="ExtractorHTML">
   </processor>
   <processor 
     name="ExtractorHTML" 
     class="org.archive.crawler.extractor.ExtractorHTML"
   next="ExtractorDOC">
  </processor>
  <processor
  	name="ExtractorDOC"
  	class="org.archive.crawler.extractor.ExtractorDOC"
  	next="ExtractorSWF">
  </processor>
  <processor
    name="ExtractorSWF"
    class="org.archive.crawler.extractor.ExtractorSWF"
    next="ExtractorPDF">
 </processor>
 <processor
   	name="ExtractorPDF"
   	class="org.archive.crawler.extractor.ExtractorPDF"
    next="Archiver">
   </processor>
   <processor 
     name="Archiver" 
     class="org.archive.crawler.basic.ARCWriter"
     next="Updater">
      <compression use="true"/>
	  <arc-files max-size-bytes="20000000"/>
	  <filter 
	   name="http-only"
	   class="org.archive.crawler.util.URIRegExpFilter"
	   regexp="^http://.*" />
   </processor>
   <processor 
     name="Updater" 
     class="org.archive.crawler.basic.CrawlStateUpdater">
   </processor>
  </processors>
 
  <limits>
   <!-- actual enforcement of these limits may depend on choice 
        of SSS/processor instances that read and respect these limits -->
   <max-link-depth value="1" /> <!-- zero means crawl seeds only -->
   <max-pages value="1000" />
   <max-duration value="1h" />
   <max-resources-per-site value="1000" />
   <max-toe-threads value="4" />
  </limits>

 </crawler-behavior>
 <disk path="example-crawl" />
</crawl-order>