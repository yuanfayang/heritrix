class-description:
Heritrix crawl order. This forms the root of the settings hierarchy.

SETTINGS_DIRECTORY-description:
Directory where override settings are kept. The settings for many modules can 
be overridden based on the domain or subdomain of the URI being processed. 
This setting specifies a file level directory to store those settings. The path
is relative to 'disk-path' unless an absolute path is provided.
# non-overrideable, expert, "settings"

DISK_PATH-description:
Directory where logs, arcs and other run time files will
be kept. If this path is a relative path, it will be
relative to the crawl order.
# non-overrideable, expert

LOGS_PATH-description:
Directory where crawler log files will be kept. If this path is a relative
path, it will be relative to the 'disk-path'.
# non-overrideable, expert, "logs"

CHECKPOINTS_PATH-description:
Directory where crawler checkpoint files will be kept. If this path is a 
relative path, it will be relative to the 'disk-path'.
# non-overrideable, expert, "checkpoints"

STATE_PATH-description:
Directory where crawler-state files will be kept. If this path is a relative 
path, it will be relative to the 'disk-path'.
# non-overrideable, expert, "state"

SCRATCH_PATH-description:
Directory where discardable temporary files will be kept. If this path is a 
relative path, it will be relative to the 'disk-path'.
# non-overrideable, expert, "scratch"

MAX_BYTES_DOWNLOAD-description:
Maximum number of bytes to download. Once this number is exceeded the crawler 
will stop. A value of zero means no upper limit.
# non-overrideable, 0L

MAX_DOCUMENT_DOWNLOAD-description:
Maximum number of documents to download. Once this number is exceeded the 
crawler will stop. A value of zero means no upper limit.
# non-overrideable, 0L

MAX_TIME_SEC-description:
Maximum amount of time to crawl (in seconds). Once this much time has elapsed 
the crawler will stop. A value of zero means no upper limit.
# non-overrideable, 0L
        
MAX_TOE_THREADS-description:
Maximum number of threads processing URIs at the same time.
# non-overrideable, 0

RECORDER_OUT_BUFFER-description:
Size in bytes of in-memory buffer to record outbound traffic. One such buffer 
is reserved for every ToeThread.
# non-overrideable, expert, 4096

RECORDER_IN_BUFFER-description:
Size in bytes of in-memory buffer to record inbound traffic. One such buffer 
is reserved for every ToeThread.
# final, expert, 65536
        
BDB_CACHE_PERCENT-description:
Percentage of heap to allocate to BerkeleyDB JE cache. Default of zero means 
no preference (accept BDB's default, usually 60%, or the je.maxMemoryPercent 
property value).
# final, expert, 0

HTTP_HEADERS-description
HTTP headers. Information that will be used when constructing the HTTP headers 
of the crawler's HTTP requests.

#
#       httpHeaders = (MapType) addElementToDefinition(new MapType(
#                ATTR_HTTP_HEADERS, "HTTP headers. Information that will " +
#                        "be used when constructing the HTTP headers of " +
#                        "the crawler's HTTP requests."));
#
#        e = httpHeaders.addElementToDefinition(new SimpleType(ATTR_USER_AGENT,
#                "User agent to act as. Field must contain valid URL " +
#                "that links to website of person or organization " +
#                "running the crawl. Replace 'PROJECT_URL_HERE' in " +
#                "initial template. E.g. If organization " +
#                "is Library of Congress, a valid user agent would be:" +
#                "'Mozilla/5.0 (compatible; loc-crawler/0.11.0 " +
#                "+http://loc.gov)'. " +
#                "Note, you must preserve the '+' before the 'http'.",
#          "Mozilla/5.0 (compatible; heritrix/@VERSION@ +PROJECT_URL_HERE)"));
#
#        e = httpHeaders.addElementToDefinition(new SimpleType(ATTR_FROM,
#                "Contact information. This field must contain a valid " +
#                "e-mail address for the person or organization responsible" +
#                "for this crawl: e.g. 'webmaster@loc.gov'",
#                "CONTACT_EMAIL_ADDRESS_HERE"));

#        addElementToDefinition(new RobotsHonoringPolicy());

#        e = addElementToDefinition(new ModuleType(
#                Frontier.ATTR_NAME, "Frontier"));
#        e.setLegalValueType(Frontier.class);

FRONTIER-description:
The frontier to use for the crawl.

RULES-description:
Ordered list of url canonicalization rules.  Rules are applied in the order
listed from top to bottom.

#        e = (MapType) addElementToDefinition(new MapType(ATTR_RULES,
#            "Ordered list of url canonicalization rules. " +
#            "Rules are applied in the order listed from top to bottom.",
#            BaseRule.class));
#        e.setOverrideable(true);
#        e.setExpertSetting(true);

#        e = addElementToDefinition(new MapType(
#                ATTR_PRE_FETCH_PROCESSORS, "Processors to run prior to" +
#                        " fetching anything from the network.",
#                        Processor.class));
#        e.setOverrideable(false);

#        e = addElementToDefinition(new MapType(
#                ATTR_FETCH_PROCESSORS, "Processors that fetch documents."
#                , Processor.class));
#        e.setOverrideable(false);

#        e = addElementToDefinition(new MapType(
#                ATTR_EXTRACT_PROCESSORS, "Processors that extract new URIs" +
#                        " from fetched documents.", Processor.class));
#        e.setOverrideable(false);

#        e = addElementToDefinition(new MapType(
#                ATTR_WRITE_PROCESSORS, "Processors that write documents" +
#                        " to archives.", Processor.class));
#        e.setOverrideable(false);

#        e = addElementToDefinition(new MapType(
#                ATTR_POST_PROCESSORS, "Processors that do cleanup and feed" +
#                        " the frontier with new URIs.", Processor.class));
#        e.setOverrideable(false);

#        loggers = (MapType) addElementToDefinition(new MapType(ATTR_LOGGERS,
#               "Statistics tracking modules. Any number of specialized " +
#               "statistics tracker that monitor a crawl and write logs, " +
#               "reports and/or provide information to the user interface."));

LOGGERS-description:
Statistics tracking modules.  Any number of specialized statistics trackers
that monitor a crawl and write logs, reports and/or provide information to
the user interface.

RECOVER_PATH-description:
Optional. Points at recover log (or recover.gz log) OR the checkpoint directory
to use recovering a crawl.
# final, expert, ""

CHECKPOINT_COPY_BDBJE_LOGS-description:
When true, on a checkpoint, we copy off the bdbje log files to the checkpoint 
directory. To recover a checkpoint, just set the recover-path to point at the 
checkpoint directory to recover.  This is default setting.  But if crawl is 
large, copying bdbje log files can take tens of minutes and even upwards of an 
hour (Copying bdbje log files will consume bulk of time checkpointing). If 
this setting is false, we do NOT copy bdbje logs on checkpoint AND we set 
bdbje to NEVER delete log files (instead we have it rename files-to-delete 
with a '.del' extension). Assumption is that when this setting is false, 
an external process is managing the removal of bdbje log files and that come 
time to recover from a checkpoint, the files that comprise a checkpoint are 
manually assembled. This is an expert setting.
            
# final, expert, true

RECOVER_RETAIN_FAILURES-description:
When recovering via the recover.log, should failures in the log be retained in 
the recovered crawl, preventing the corresponding URIs from being retried. 
Default is false, meaning failures are forgotten, and the corresponding URIs 
will be retried in the recovered crawl.

# final, expert, false

        
#        e = addElementToDefinition(
#           new CredentialStore(CredentialStore.ATTR_NAME));
#        e.setOverrideable(true);
#        e.setExpertSetting(true);

CREDENTIAL_STORE-description:
The credential store.