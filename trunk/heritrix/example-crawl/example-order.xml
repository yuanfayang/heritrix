<crawl-order name="example-crawl">
 <comment>
  A simple crawl for example purposes.
 </comment>
 
 <crawler-behavior>

  <http-headers>
   <User-Agent>heritrix-aoc/alpha (+http://crawler.archive.org)</User-Agent>
   <From>archive-crawler-agent@lists.sourceforge.net</From>
  </http-headers>
 
  <selector class="org.archive.crawler.basic.SimpleSelector">
   <seeds>
   # http://my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/&amp;.confirm=1&amp;.done=http:/my.yahoo.com/p/ldep
   # http://dmoz.org
   # http://www.yahoo.com
   # http://www.msnbc.com
   # http://www.lycos.com
   # http://www.drudgereport.com
# http://www.army.mod.uk
# http://www.dfid.gov.uk
# http://www.fco.gov.uk
# http://www.mod.uk
# http://www.odpm.gov.uk
# http://www.pm.gov.uk
# http://www.raf.mod.uk
# http://www.royal-navy.mod.uk
# http://www.sabre.mod.uk
# http://www.archive.org/..
#http://www.yahoo.com/../../movies
#http://www.archive.org/movies/fake/../movies.php
#http://www.archive.org/movies/../../blahblah
#http://www.archive.org/movies/../../../../somethingelse
#http://www.army.mod.uk/ceremonialandheritage/museums_main.htm
# http://www.creativecommons.org/../
#http://www.royal-navy.mod.uk/rn/form/form.html?page=1
#http://www.dfid.gov.uk/../../aboutdfid/files/glossary_l.htm
#http://directory.google.com/Top/Games/
# http://www3.google.com/help/customize.html
http://dmoz.org

   </seeds>
   <!--
   <filter 
    name="yahoo" 
    class="org.archive.crawler.util.URIRegExpFilter"
    regexp=".*yahoo\.com.*" />
    -->
    <!--
   <filter 
    name="pathological-path" 
    class="org.archive.crawler.util.URIRegExpFilter"
    modifier="not"
    regexp=".*/([^/]*)/\1/\1/.*" />
   <filter 
    name="no-more-than-20-slashes" 
    class="org.archive.crawler.util.URIRegExpFilter"
    modifier="not"
    regexp="[^/]*?//[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?/[^/]*?" />
   -->
    <!--
    <filter
    name="problemarea"
    class="org.archive.crawler.util.URIRegExpFilter"
    regexp="http://www\.royal-navy\.mod\.uk/rn/form/form\.html.*" />
    <filter
    name="within-8hosts"
    class="org.archive.crawler.util.URIRegExpFilter"
    regexp="http://www\.((army\.mod\.uk)|(dfid\.gov\.uk)|(fco\.gov\.uk)|(mod\.uk
)|(odpm\.gov\.uk)|(pm\.gov\.uk)|(raf\.mod\.uk)|(royal-navy\.mod\.uk)|(sabre\.mod
\.uk)).*" />
    -->

  </selector>
  
  <scheduler class="org.archive.crawler.basic.SimpleScheduler" />
  
  <store class="org.archive.crawler.basic.SimpleStore" />
 
  <processors>
   <processor 
     name="Preselector" 
     class="org.archive.crawler.basic.SimplePreselector"
     next="Preprocessor">
    <params max-link-depth="1" max-embed-depth="2" />
    <filter
     name="focus"
     class="org.archive.crawler.util.SeedExtensionFilter"
     mode="domain"
     />
   </processor>
   <processor 
     name="Preprocessor" 
     class="org.archive.crawler.basic.SimplePreconditionEnforcer"
     next="DNS">
    <params delay-factor="3" minimum-delay="100" />
   </processor>
   <processor 
     name="DNS" 
     class="org.archive.crawler.basic.FetcherDNS"
     next="HTTP">
   </processor>
   <processor 
     name="HTTP" 
     class="org.archive.crawler.basic.FetcherHTTPSimple"
     next="ExtractorHTTP">
     <params timeout-seconds="10" />
   </processor>
   <processor 
     name="ExtractorHTTP" 
     class="org.archive.crawler.extractor.ExtractorHTTP"
     next="ExtractorHTML">
   </processor>
   <processor 
     name="ExtractorHTML" 
     class="org.archive.crawler.extractor.ExtractorHTML"
   next="ExtractorDOC">
  </processor>
  <processor
  	name="ExtractorDOC"
  	class="org.archive.crawler.extractor.ExtractorDOC"
  	next="ExtractorSWF">
  </processor>
  <processor
    name="ExtractorSWF"
    class="org.archive.crawler.extractor.ExtractorSWF"
    next="ExtractorPDF">
 </processor>
 <processor
   	name="ExtractorPDF"
   	class="org.archive.crawler.extractor.ExtractorPDF"
    next="Archiver">
   </processor>
   <processor 
     name="Archiver" 
     class="org.archive.crawler.basic.ARCWriter"
     next="Updater">
      <compression use="true"/>
	  <arc-files max-size-bytes="20000000"/>
	  <!--
	  <filter 
	   name="http-only"
	   class="org.archive.crawler.util.URIRegExpFilter"
	   regexp="^http://.*" />
	   -->
   </processor>
   <processor 
     name="Updater" 
     class="org.archive.crawler.basic.CrawlStateUpdater">
   </processor>
  </processors>
 
  <limits>
   <!-- actual enforcement of these limits may depend on choice 
        of SSS/processor instances that read and respect these limits -->
   <max-link-depth value="100" /> <!-- zero means crawl seeds only -->
   <max-embed-depth value="5" /> <!-- extra hops that can be taken for embeds -->
   <max-toe-threads value="20" />
  </limits>

 </crawler-behavior>
 <disk path="example-crawl" />
 
 <loggers>
  <crawl-statistics>
    <interval>10</interval>
  </crawl-statistics>
</loggers>

</crawl-order>