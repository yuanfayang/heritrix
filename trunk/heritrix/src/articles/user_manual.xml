<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">
<article>
  <title>Heritrix User Manual</title>

  <sect1>
    <title>Introduction</title>

    <para>Heritrix is the Internet Archive's open-source, extensible,
    web-scale, archival-quality web crawler. </para>

    <para>This document explains how to create, configure and run crawls using
    it. It is intended for users of the software and presumes that they
    possess at least a general familiarity with the concept of web crawling.
    </para>
  </sect1>

  <sect1>
    <title>Installing and running Heritrix</title>

    <para>This chapter will explain how to set up Heritrix. Some basic Linux
    administration skills are needed for this. </para>

    <para>Because Heritrix is a pure Java program it can (in theory anyway) be
    run on any platform that has a Java 1.4 VM. However we are only committed
    to supporting it's operation on Linux and so this chapter only covers
    setup on that platform. Other chapters in the user manual are entirely
    platform agnostic.</para>

    <para>This chapter also only covers installing and running the prepackaged
    binary distributions of Heritrix. For information about downloading and
    compiling the source see the Developers manual.</para>

    <sect2>
      <title>Obtaining and installing Heritrix</title>

      <para>The packaged binary can be downloaded from the project's <ulink
      url="http://sourceforge.net/projects/archive-crawler">sourceforge home
      page</ulink>. Each release comes in four flavors, packaged as .tar.gz or
      .zip and including source or not.</para>

      <para>For installation on linux get the file
      <filename>heritrix-?.?.?.tar.gz</filename> (where ?.?.? is the most
      recent version number).</para>

      <para>The packaged binary comes largely ready to run. Once downloaded it
      can be untarred into the desired directory.</para>

      <para><programlisting>  % tar xfz heritrix-?.?.?.tar.gz</programlisting></para>

      <para>Once you have downloaded and untarred the correct file you can
      move on to the next step.</para>
    </sect2>

    <sect2>
      <title>Installing Java</title>

      <para>Before you can install Heritrix you must ensure that a <emphasis
      role="bold">JavaVM 1.4</emphasis> (or later) is installed on the
      machine. Heritrix comes bundled with all libraries needed to run it. The
      only prerequisite is the JavaVM.</para>

      <para>If you already have Java installed you can move on the next step
      if not you can download Java from:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Sun</emphasis> -- <ulink
          url="http://java.sun.com/">java.sun.com</ulink> </para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">IBM</emphasis> -- <ulink
          url="http://www.ibm.com/java">www.ibm.com/java</ulink></para>
        </listitem>
      </itemizedlist>

      <para>It has been our experiance that running on Linux with the IBM Java
      virtual machine Heritrix performs better then with the Sun JavaVM (Java
      version 1.4.2). This may change in the future. There are no known
      problems with using either of the two.</para>
    </sect2>

    <sect2>
      <title>Running Heritrix</title>

      <para> To run Heritrix, first do the following:<programlisting>  % export HERITRIX_HOME=/PATH/TO/BUILT/HERITRIX</programlisting>...where
      <literal>$HERITRIX_HOME</literal> is the location of your untarred
      <filename>heritrix.?.?.?.tar.gz</filename>.</para>

      <para>Next run:<programlisting>  % cd $HERITRIX_HOME
  % chmod u+x $HERITRIX_HOME/bin/heritrix
  % $HERITRIX_HOME/bin/heritrix --help</programlisting>This should give you
      usage output like the following:</para>

      <para><programlisting><computeroutput>  Usage: heritrix --help
  Usage: heritrix --nowui ORDER_FILE
  Usage: heritrix [--port=PORT] [--admin=LOGIN:PASSWORD] [--run] [ORDER_FILE]
  Usage: heritrix [--port=PORT] --selftest[=TESTNAME]
  Version: @VERSION@
  Options:
   -a,--admin      Login and password for web user interface administration.
                   Default: admin/letmein.
   -h,--help       Prints this message and exits.
   -n,--nowui      Put heritrix into run mode and begin crawl using ORDER_FILE. Do
                   not put up web user interface.
   -p,--port       Port to run web user interface on.  Default: 8080.
   -r,--run        Put heritrix into run mode. If ORDER_FILE begin crawl.
   -s,--selftest   Run the integrated selftests. Pass test name to it only (Case
                   sensitive: E.g. pass 'Charset' to run charset selftest).
  Arguments:
   ORDER_FILE     Crawl order to run.</computeroutput></programlisting>Launch
      the crawler with the UI enabled by doing the following:</para>

      <para><programlisting>  % $HERITRIX_HOME/bin/heritrix</programlisting>This
      will start up heritrix printing out a startup message that looks like
      the following:</para>

      <para><programlisting>  [b116-dyn-60 619] heritrix-0.4.0 &gt; ./bin/heritrix
  Tue Feb 10 17:03:01 PST 2004 Starting heritrix...
  Tue Feb 10 17:03:05 PST 2004 Heritrix 0.4.0 is running.
  Web UI is at: http://b116-dyn-60.archive.org:8080/admin
  Login and password: admin/letmein</programlisting></para>

      <para>The only command line options that you may want to use right away
      are:</para>

      <itemizedlist>
        <listitem>
          <para><literal>--port=PORT</literal> </para>

          <para>Set what port the web based user interface runs on. </para>
        </listitem>

        <listitem>
          <para><literal>--admin=LOGIN:PASSWORD</literal> </para>

          <para>Change the default admin username and password. If you do not
          do this then the defaul username and password will be in effect.
          Since they are widely known that may not be desirable.</para>
        </listitem>
      </itemizedlist>

      <para>See <xref linkend="wui" /> and <xref linkend="tutorial" /> to get
      your first crawl up and running.</para>
    </sect2>
  </sect1>

  <sect1 id="wui">
    <title>Web based user interface</title>

    <para>After Heritrix has been launched from the command line, the web
    based user interface (WUI) becomes accessible.</para>

    <para>The path to it printed out on the console from which the program was
    launched (typically http://&lt;host&gt;:8080/admin/).</para>

    <para>The WUI is password protected. The username is currently hardcoded
    to 'admin' with the default password of 'letmein'. The password can be
    specified at startup and users are encouraged to change it. The currently
    valid password will be printed out to the console along with the
    path.</para>

    <para>The WUI can then be accessed via any browser. While we've
    endeavoured to make certain that it functions in all recent browsers,
    Mozilla 5+ is recommended. IE 6+ should also work without problems.</para>

    <para>The initial login page takes the standard username/password
    combination discussed before. In addition users can opt to have the WUI
    remember their login by checking the appropriate box. If that is done then
    the login information will be stored in a cookie and read from it during
    future login attempts. If this option is not checked then the login is
    only valid as long as the server's session. Once it times you, the user
    will need to login again.</para>

    <caution>
      <para>The access control to the WUI is not encrypted! Passwords will be
      submitted over the network in plain text.</para>
    </caution>
  </sect1>

  <sect1 id="tutorial">
    <title>A quick guide to running your first crawl job</title>

    <para></para>
  </sect1>

  <sect1>
    <title>Creating jobs and profiles</title>

    <para>In order to run a crawl a configuration must be created that defines
    it. In Heritrix such a configuration is called a Crawl job.</para>

    <sect2>
      <title id="crawljob">Crawl job</title>

      <para>A Crawl job encompasses the configurations needed to run a single
      crawl. It also contains some additional elements such as logs, status
      etc.</para>

      <para>Once logged onto the WUI new jobs can be created by going to the
      <emphasis>Jobs</emphasis> tab. Once the Jobs page loads users can create
      jobs by choosing of the following three options:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Create new crawl job</emphasis></para>

          <para>This option creates a new crawl job based on the currently set
          default profile.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Create new crawl job based on a
          profile</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          one of the profiles offered up.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Create new crawl job based on an
          existing job</emphasis></para>

          <para>This option allows the user to create a job by basing it on
          any existing job, regardless of whether it has been crawled or not.
          Can be useful for repeating crawls..</para>
        </listitem>
      </orderedlist>

      <para>Options 2 and 3 will display a list of available options.
      Initially there is only one profile and no existing jobs.</para>

      <para>As should be clear by now, all crawl jobs are created by basing
      them on profiles (see next chapter) or existing jobs.</para>

      <para>Once the proper profile/job has been chosen to base the new job
      one a simple page will appear asking for the new jobs:</para>

      <orderedlist>
        <listitem>
          <para><emphasis role="bold">Name</emphasis></para>

          <para>The name must only contain letters, numbers, dash (-) and
          underscore (_). No other characters are allowed. This name will be
          used to identify the crawl in the WUI but it need not be unique. The
          name can not be changed later</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Description</emphasis></para>

          <para>A short description of the job. This can be edited
          later.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Seeds</emphasis></para>

          <para>The seed URLs to use for the job. This list can be edited
          later along with the general configurations.</para>
        </listitem>
      </orderedlist>

      <para>Below these input fields there are several buttons. The last one
      Submit job will immediately submit the job and (assuming that it's
      properly configured) it will be ready to run (see <xref
      linkend="running" />). The other buttons will take the user to the
      relevant configuration pages (those are covered in detail in <xref
      linkend="config" />). Once all desired changes have been made to the
      configuration, click the '<emphasis>Submit job</emphasis>' tab (usually
      displayed top and bottom right) to submit it to the list of waiting
      jobs.<note>
          <para>Changes made afterwards to the original jobs or profiles that
          a new job is based on will <emphasis role="bold">not</emphasis> in
          any way affect the newly created job.</para>
        </note></para>
    </sect2>

    <sect2>
      <title id="profile">Profile</title>

      <para>It's best to think of a profile as a template for a crawl job. It
      contains all the configurations that a crawl job would, but is not
      considered to be 'crawlable'. That is Heritrix will not allow you to
      directly crawl a profile, only jobs based on profiles. The reason for
      this is that while profiles may in fact be complete, they may not
      be.</para>

      <para>A common example is leaving the HTTP headers
      (<emphasis>user-agent</emphasis>, <emphasis>from</emphasis>) in an
      illegal state in a profile to force the user to input valid data. This
      applies to the default (<emphasis>Simple</emphasis>) profile that comes
      with Heritrix. Other examples would be leaving the seeds list empty, not
      specifying some processors (such as the writer/indexer) etc.</para>

      <para>In general there is less error checking of profiles.</para>

      <para>To manage profiles, go to the <emphasis>Profiles</emphasis> tab in
      the WUI. That page will display a list of existing profiles. To create a
      new profile select the option of creating a "New profile based on it"
      from the existing profile to use as a template. Much like jobs, profiles
      can only be created based on other profiles. It is not possible to
      create profiles based on existing jobs.</para>

      <para>The process from there on mirrors the creating of jobs. A page
      will ask for the new profiles name, description and seeds list. Unlike
      job names, profile names <emphasis>must be unique</emphasis> from other
      profile names - jobs and a profile can share the same name - otherwise
      the same rules apply.</para>

      <para>The user then proceeds to the configuration pages (see <xref
      linkend="config" />) to modify the behavior of the new profile from that
      of the parent profile.<note>
          <para>Even though profiles are based on other profiles, changes made
          to the original profiles afterwards will <emphasis
          role="bold">not</emphasis> affect the new ones.</para>
        </note></para>
    </sect2>
  </sect1>

  <sect1 id="config">
    <title>Configuring jobs and profiles</title>

    <para>Creating crawl jobs (<xref linkend="crawljob" />) and profiles
    (<xref linkend="profile" />) is just the first step. Configuring them is a
    more complicated process.</para>

    <para>The following section applies equally to configuring crawl jobs and
    profiles. It does not matter if new ones are being created or existing
    ones are being edited. The interface is almost entirely the same, only the
    '<emphasis>Submit job</emphasis>'/'<emphasis>Finished</emphasis>' button
    will vary slightly.</para>

    <para>Each page in the configuration section of the WUI will have a
    secondary row of tabs below the system ones. This secondary row is often
    replicated at the bottom of longer pages.</para>

    <para>This row offers access to different parts of the configuration.
    While configuring the global level (more on global vs. overrides and
    refinements in <xref linkend="overrides" /> and <xref
    linkend="refinements" />) the following options are available:</para>

    <itemizedlist>
      <listitem>
        <para>Modules (<xref linkend="modules" />)</para>

        <para>Add/remove/set configurable modules</para>
      </listitem>

      <listitem>
        <para>Filters (<xref linkend="filters" />)</para>

        <para>Add/remove/reorder filters</para>
      </listitem>

      <listitem>
        <para>Credentials (<xref linkend="credentials" />)</para>

        <para>Add/remove login credentials</para>
      </listitem>

      <listitem>
        <para>Settings (<xref linkend="settings" />)</para>

        <para>Configure settings on Heritrix modules</para>
      </listitem>

      <listitem>
        <para>Overrides (<xref linkend="overrides" />)</para>

        <para>Override settings on Heritrix modules based on domain</para>
      </listitem>

      <listitem>
        <para>Refinements (<xref linkend="refinements" />)</para>

        <para>Refine settings on Heritrix modules based on arbitrary
        criteria</para>
      </listitem>

      <listitem>
        <para>Submit job / Finished</para>

        <para>Clicking this tab will take the user back to the Jobs or
        Profiles page, saving any changes.</para>
      </listitem>
    </itemizedlist>

    <para>The 'Settings' tab is probably the most frequently used page as it
    allows the user to fine tune the settings of any Heritrix module used in
    the job/profile.</para>

    <para>It is safe to navigate between these, it will not cause new jobs to
    be submitted to the queue of pending jobs. That only happens once the
    Submit job tab is clicked. Navigating out of the configuration pages using
    the top level tabs will cause new jobs to be lost. Any changes made are
    saved when navigating within the configuration pages. There is no undo
    function, once made, changes can not be undone.</para>

    <sect2 id="modules">
      <title>Modules</title>

      <para>Heritrix has several types of pluggable modules. These modules,
      while having a fixed interface and a variable number of provided
      implementation can also be third party plugins. This page allows the
      user to select several types of these pluggable modules.</para>

      <para>Once modules have been added to the configuration they can be
      configured in greater detail on the Settings page (<xref
      linkend="settings" />).</para>

      <para><note>
          <para>Modules are referred to by their Java class names
          (org.archive.crawler.frontier.Frontier). This is done because these
          are the only names we can be assured of being unique.</para>
        </note>See Developer manual for information about creating and adding
      custom modules to Heritrix.</para>

      <sect3>
        <title>URI Frontier</title>

        <para>The URI Frontier is a module that maintains the internal state
        of the crawl. What URIs have been discovered, crawled etc. As such
        it's selection greatly affects for instance the order in which
        discovered URIs are crawled.</para>

        <para>There is only one Frontier per crawl.</para>

        <para>At present only one Frontier is provided with Heritrix. It
        offers some options to influence it between site first and breadth
        first crawling.</para>
      </sect3>

      <sect3>
        <title>Crawl Scope</title>

        <para>A crawl scope is an object that decides for each discovered URI
        if it is within the scope of the current crawl.</para>

        <para>Several scopes are provided with Heritrix:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">BroadScope</emphasis></para>

            <para>This scope allows for limiting the depth of a crawl (how
            many links away Heritrix should crawl) but does not impose any
            limits on which domains or hosts discovered URIs must belong
            to.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">DomainScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of domains
            defined by the provided seeds. That is any URI discovered
            belonging to a domain from which one of the seed came is within
            scope. Like always it is possible to apply depth
            restrictions.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">HostScope</emphasis></para>

            <para>This scope limits discovered URIs to the set of hosts
            defined by the provided seeds.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PathScope</emphasis></para>

            <para>This scope goes yet further and limits the discovered URIs
            to a section of paths on hosts defined by the seeds. Of course any
            host that has a seed pointing at it's root (i.e.
            www.sample.com/index.html) will be included in full where as a
            host whose only seed is www.sample2.com/path/index.html will be
            limited to URIs under /path/.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">FilterScope</emphasis></para>

            <para>A highly configurable scope. By adding different filters in
            different combinations this scope can be configured to provide a
            wide variety of behaviour.</para>
          </listitem>
        </itemizedlist>

        <para>Scopes usually allow for some flexibility in defining depth and
        possible transitive includes (that is getting items that would usually
        be out of scope because of special circumstance such as them being
        embedded). Most noticeably they can have filters applied to themselves
        in two different contexts (some filters may only have one these
        contexts).</para>

        <orderedlist>
          <listitem>
            <para><emphasis role="bold">Focus</emphasis></para>

            <para>URIs matching these filters will be considered to be within
            scope</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Exclude</emphasis></para>

            <para>URIs matching these filters will be considered to be out of
            scope.</para>
          </listitem>
        </orderedlist>

        <para>Custom made Scopes may have different set of filters. Also some
        scopes have filters hardcoded into them. This allows you to edit their
        settings but not remove or replace them. For example most of the
        provided scopes have a <literal>Transclusion</literal> filter
        hardcoded into them that handles transitive items (URIs that normally
        shouldn't be included but because of special circumstance they will be
        included). </para>

        <para>For more about Filters see <xref linkend="filters" />.</para>

        <sect4 id="scopeproblems">
          <title>Problems with the current Scopes</title>

          <para>Up until now, our predefined Scope classes -- PathScope,
          HostScope, DomainScope, BroadScope -- all could be thought of as
          fitting a specific pattern: A URI is included if and only if:</para>

          <para><programlisting>    ( ( focusFilter.accepts(u)
        || transitiveFilter.accepts(u) )
      &amp;&amp; exclusionFilter.accepts(u) == false )</programlisting></para>

          <para>More generally, the <emphasis>focus</emphasis> filter was
          meant to rule things in by prima facia/regexp-pattern analysis; the
          <emphasis>transitive</emphasis> filter rule extra items in by
          dynamic path analysis (for example, off site embedded images); and
          the <emphasis>exclusion</emphasis> filter rule things out by any
          number of chained exclusion rules. So in a typical crawl, the
          <emphasis>focus</emphasis> filter drew from one of these
          categories:<itemizedlist>
              <listitem>
                <para><emphasis role="bold">broad</emphasis> : accept
                all</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">domain</emphasis>: accept if on
                same 'domain' (for some definition) as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">host</emphasis>: accept if on
                exact host as seeds</para>
              </listitem>

              <listitem>
                <para><emphasis role="bold">path</emphasis>: accept if on same
                host and a shared path-prefix as seeds</para>
              </listitem>
            </itemizedlist>The <emphasis>transitive</emphasis> filter was
          configured based on the various link-hops and embed-hops thresholds
          set by the operator.</para>

          <para>The <emphasis>exclusion</emphasis> filter was in fact a
          compound chain of filters, OR'ed together, such that any one of them
          could knock a URI out of consideration. However, a number of aspects
          of this arrangement have caused problems: <orderedlist>
              <listitem>
                <para>To truly understand what happens to an URI, you must
                understand the above nested boolean-construct.</para>
              </listitem>

              <listitem>
                <para>Adding mixed focuses -- such as all of this one host,
                all of this other domain, and then just these paths on this
                other host -- is not supported, nor easy to mix-in to the
                <emphasis>focus</emphasis> filter.</para>
              </listitem>

              <listitem>
                <para>Constructing and configuring the multiple filters
                required many setup steps across several WUI pages.</para>
              </listitem>

              <listitem>
                <para>The reverse sense of the <emphasis>exclusion</emphasis>
                filters -- if URIs are accepted by the filter, they are
                excluded from the crawl -- proved confusing, exacerbated by
                the fact that 'filter' itself can commonly mean either 'filter
                in' or 'filter out'.</para>
              </listitem>
            </orderedlist></para>

          <para>As a result of these problems major changes are planed in post
          1.0.0 versions of Heritrix. The concept of scope will remain but how
          they are defined and configured will change drastically . Those
          changes will also greatly affect the filters (see <xref
          linkend="filters" />).</para>
        </sect4>
      </sect3>

      <sect3>
        <title id="processingchains">Processing Chains</title>

        <para>When a URI is crawled it is in fact passed through a series of
        processors. This series is split up into five chains and the user can
        add, remove and reorder the processors on each of these chains.</para>

        <para>While generally each processor only makes sense in one of these
        chains that is not currently enforced.</para>

        <para>Each URI taken off the Frontier queue runs through the
        <literal>Processing Chains</literal> listed in the diagram shown on
        the right. URIs are always processed in the order shown in the diagram
        unless a particular processor throws a fatal error. In this
        circumstance, processing skips to the end, to the Post-processing
        chain, for cleanup.</para>

        <para>Each processing chain is made up of zero or more individual
        processors. For example, the extractor processing chain might comprise
        the <literal>ExtractorHTML</literal> , an
        <literal>ExtractorJS</literal> , and the
        <literal>ExtractorUniversal</literal> processors. Within a processing
        step, the order in which processors are run is the order in which
        processors are listed in the job order file.</para>

        <para>Generally, particular processors only make sense with in the
        context of one particular processing chain. For example, it wouldn't
        make sense to run the <literal>FetchHTTP</literal> processor in the
        Post-processing chain.</para>

        <graphic fileref="processing_steps.png" format="PNG" />

        <para>Most of the processors are fairly self explanatory, however the
        first and last two merit a little bit more attention.</para>

        <para>In the <literal>Pre Processing</literal> chain the following two
        processors should be included (or replacement modules that perform
        similar operations:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Preselector</emphasis></para>

            <para>Last check if the URI should indeed be crawled. Can for
            example recheck scope. Useful if it has been changed after the
            crawl starts.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">PreconditionEnforcer</emphasis></para>

            <para>Ensures that all preconditions for crawling a URI have been
            met. These currently include verifying that DNS and robots.txt
            information has been fetched for the URI.</para>
          </listitem>
        </itemizedlist>

        <para>Similarly the <literal>Post Processing</literal> chain has to
        special purpose processors:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">CrawlStateUpdater</emphasis></para>

            <para>Updates the per-host information that may have been affected
            by the fetch. This is currently robots and IP address info.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Postselector</emphasis></para>

            <para>Feeds discovered URIs back to the Frontier. Without this
            processor no discovered URI will ever be scheduled for
            crawling.</para>
          </listitem>
        </itemizedlist>

        <para>Within each chain the processors will be executed in the order
        in which they are displayed on the settings page.</para>
      </sect3>

      <sect3>
        <title>Statistics Tracking</title>

        <para>Any number of statistics tracking modules can be attached to a
        crawl. Currently only one is provided with Heritrix. The
        <literal>StatisticsTracker</literal> module that comes with Heritrix
        writes the progress-statistics.log file and provides the WUI with the
        data it needs to display progress information about a crawl. It is
        strongly recommended that any crawl running with the WUI use this
        module.</para>
      </sect3>
    </sect2>

    <sect2 id="filters">
      <title>Filters</title>

      <para></para>
    </sect2>

    <sect2 id="credentials">
      <title>Credentials</title>

      <para>In this section you can add login credentials that will allow
      Heritrix to gain access to areas of websites requiring authentication.
      As with all modules they are only added here (supplying a unique name
      for each credential) and then configured on the settings page (<xref
      linkend="settings" />).</para>

      <para>One of the settings for each credential is it's
      <literal>credential-domain</literal> and thus it is possible to create
      all credentials on the global level. However since this can cause
      excessive unneeded checking of credentials it is recommended that
      credentials be added to the appropriate domain override (see <xref
      linkend="overrides" /> for details). That way the credential is only
      checked when the relevant domain is being crawled.</para>

      <para>Heritrix can do two types of authentication: <ulink
      url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC and
      DIGEST Auth) and POST and GET of an HTML Form.</para>

      <note>
        <title>Logging</title>

        <para>To enable logging of authentication interactions, set the
        FetchHTTP log level to fine</para>

        <para><programlisting>org.archive.crawler.fetcher.FetchHTTP.level = FINE</programlisting></para>

        <para>This is done by editing the heritrix.properties file under the
        conf directory.</para>
      </note>

      <sect3>
        <title><ulink
        url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink> (BASIC and
        DIGEST Auth)</title>

        <para>Supply <ulink url="#cd">Credential Domain</ulink>, <ulink
        url="realm">Realm</ulink>, login, and password.</para>

        <para>The way that the RFC2617 authentication works in Heritrix is
        that in response to a 401 response code (Unauthorized), Heritrix will
        use a key made up of the Credential Domain plus Realm to do a lookup
        into its Credential Store. If a match is found, then the credential is
        loaded into the CrawlURI and the CrawlURI is marked for immediate
        retry.</para>

        <para>When the requeued CrawlURI comes around again, this time
        through, the found credentials are added to the request. If the
        request succeeds -- result code of 200 -- the credentials are promoted
        to the CrawlServer and all subsequent requests made against this
        CrawlServer will preemptively volunteer the credential. If the
        credential fails -- we get another 401 -- then the URI is let die a
        natural 401 death.</para>

        <sect4 id="cd">
          <title>Credential Domain</title>

          <para>This equates to the canonical root URI of RFC2617;
          effectively, in our case, its the CrawlServer name or <ulink
          url="http://java.sun.com/j2se/1.4.2/docs/api/java/net/URI.html">URI
          authority</ulink> (domain plus port if other than port 80).</para>
        </sect4>

        <sect4 id="realm">
          <title>Realm</title>

          <para>Realm as per <ulink
          url="http://www.faqs.org/rfcs/rfc2617.html">RFC2617</ulink>. The
          realm string must match exactly the realm name presented in the
          authentication challenge served up by the web server</para>
        </sect4>

        <sect4>
          <title>Known Limitations</title>

          <sect5>
            <title>One Realm per Credential Domain Only</title>

            <para>Currently, you can only have one realm per credential
            domain.</para>
          </sect5>

          <sect5>
            <title>Digest Auth works for Apache</title>

            <para>... but your mileage may vary going up against other servers
            (See <ulink
            url="http://sourceforge.net/tracker/index.php?func=detail&amp;aid=914301&amp;group_id=73833&amp;atid=539102">[
            914301 ] Logging in (HTTP POST, Basic Auth, etc.)</ulink> to learn
            more).</para>
          </sect5>
        </sect4>
      </sect3>

      <sect3>
        <title>HTML Form POST or GET</title>

        <para></para>
      </sect3>
    </sect2>

    <sect2 id="settings">
      <title>Settings</title>

      <para>This page presents a semi treelike representation of all the
      modules (fixed and pluggable alike) that make up the current
      configuration and allows the user to edit any of their settings.</para>

      <para>The first option presented directly under the top tabs is whether
      or to hide or display 'expert settings'. Expert settings are those
      settings that are rarely changed and should only be changed by someone
      with a clear understanding of their implication. This document will not
      discuss any of the expert settings.</para>

      <para>The first setting is the description of the job previously
      discussed. The seed list is at the bottom of the page. Between the two
      are all the other possible settings.</para>

      <para>Module names are presented in bold and a short explanation of them
      is provided. As discussed in the previous three chapters some of them
      can be replaced, removed or augmented.</para>

      <para>Behind each module and settings name a small question mark is
      present. By clicking on it a more detailed explanation of the relevant
      item pops up. For most settings users should refer to that as their
      primary source of information.</para>

      <para>Some settings provide a fixed number of possible 'legal' values in
      combo boxes. Most are however typical text input fields. Two types of
      settings require a bit of additional attention.</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Lists</emphasis></para>

          <para>Some settings are a list of values. In those cases a list is
          printed with an associated remove button and an input box is printed
          below it with an add button. Only those items in the list box are
          considered in the list itself. A value in the input box does not
          become a part of the list until the user clicks add. There is no way
          to edit existing values beyond removing them and replacing them with
          correct values. It is also no possible to reorder the list.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Simple typed maps</emphasis></para>

          <para>Generally Maps in the Heritrix settings framework contain
          program modules (such as the processors for example) and are
          therefor edited elsewhere. However maps that only accept simple data
          types (Java primitives) can be edited here.</para>

          <para>They are treated as a key, value pair. Two input boxes are
          provided for new entries with the first one representing the key and
          the second the value. Clicking the associated add button adds the
          entry to the map. Above the input boxes a list of existing entries
          is displayed along with a remove option. Simple maps can not be
          reordered.</para>
        </listitem>
      </itemizedlist>

      <para>Changes on this page are not saved until you navigate to another
      part of the settings framework or you click the submit job/finished
      tab.</para>

      <para>If there is a problem with one of the settings a red star will
      appear next to it. Clicking the star will display the relevant error
      message.</para>
    </sect2>

    <sect2 id="overrides">
      <title>Overrides</title>

      <para>Overrides provide the ability to override individual settings on a
      per domain bases. The overrides page provides an iterative list of
      domains that contain override settings, that is values for parameters
      that override values in the global configuration.</para>

      <para>It is best to think of the general global settings as the root of
      the settings hierarchy and they are then overridden by top level domains
      (com, net, org, etc) who are in turn overridden by domains (yahoo.com,
      archive.org, etc.) who can further be overridden by sub domains
      (crawler.archive.org). There is no limit for how deep into the sub
      domains the overrides can go.</para>

      <para>When a URI is being processed the settings for it's host is then
      first lookup up. If the needed setting is not available there it's super
      domains are checked until the settings is found (all settings exist at
      the global level at the very least).</para>

      <para>Creating a new override is done by simply typing in the domain in
      the input box at the bottom of the page and clicking the 'Create/Edit'
      button. Alternatively if overrides already exist the user can navigate
      the hierarchy of existing overrides, edit them and create new overrides
      on domains that don't already have them.</para>

      <para>Once an override has been created or selected for editing the user
      is taken to a page that closely resembles the settings page discussed in
      <xref linkend="settings" />. The main difference is that those settings
      that can not be overridden (file locations, number of threads etc.) are
      printed in a non-editable manner. Those settings that can be edited now
      have a checkbox in front of them. If they are being overridden at the
      current level that checkbox should be checked. Editing a setting will
      cause the checkmark to appear. Removing the checkmark effectively
      removes the override on that setting.</para>

      <para>Once on the settings page the second level tabs will change to
      override context. The new tabs will be similar to the general tabs and
      will have:</para>

      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Filters</emphasis></para>

          <para>Add filters to the override. It is not possible to remove
          inherited filters or interject new filters among them. New filters
          will be added after existing filters. All filters though have to
          option to disable them that can be set on the settings page.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Credentials</emphasis></para>

          <para>Add credentials to the override. Generally credentials should
          always be added to an override of the domain most relevant to them.
          See <xref linkend="credentials" /> for more details.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Settings</emphasis></para>

          <para>Page allowing the user to override specific settings as
          discussed above.</para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Refinements</emphasis></para>

          <para>Manage refinements for the override. See <xref
          linkend="refinements" /></para>
        </listitem>

        <listitem>
          <para><emphasis role="bold">Done with override</emphasis></para>

          <para>Once the user has finished with the override, this option will
          take him back to the overrides overview page.</para>
        </listitem>
      </itemizedlist>

      <para>It is not possible to add, remove or reorder existing modules on
      an override. It is only possible to add filters and credentials. Those
      added will be inherited to sub domains of the current override domain.
      Those modules that are added in an override will not have a checkbox in
      front of their settings on the override settings page since the override
      is effectively their 'root' domain.</para>

      <para>Finally, due to how the settings framework is structured there is
      negligible performance penalty to using overrides. Lookups for settings
      take as much time whether or not overrides have been defined. For URIs
      belonging to domains without overrides no performance penalty is
      incurred.</para>
    </sect2>

    <sect2 id="refinements">
      <title>Refinements</title>

      <para>Refinements are similar to overrides (see <xref
      linkend="overrides" />) in that the allow the user to modify the
      settings under certain circumstances. There are however two major
      differences.</para>

      <orderedlist>
        <listitem>
          <para>Refinements are applied based on arbitrary criteria rather
          then encountered URIs domain.</para>

          <para>Currently it is possible to set criteria based on the time of
          day, a regular expression matching the URI and the port number of
          the URI.</para>
        </listitem>

        <listitem>
          <para>They incur a performance penalty.</para>

          <para>This effect is small if there numbers are few but for each URI
          encountered there must be a check made to see if it matches any of
          the existing criteria of defined refinements.</para>

          <para>This effect can be mitigated by applying refinements to
          overrides rather then the global settings.</para>
        </listitem>
      </orderedlist>

      <para>Refinements can be applied either to the global settings or to any
      override. If applied to an override they can affect any settings,
      regardless of whether the parent override has modified it.</para>

      <para>It is not possible to create refinements on refinements.</para>

      <para>Clicking the refinements tab on either the global settings or an
      override brings the user to the refinements overview page. The overview
      page displays a list of existing refinements on the current level and
      allows the user to create new ones.</para>

      <para>To create a new override the user must supply a unique name for it
      (name is limited to letters, numbers, dash and underscore) and a short
      description that will be displayed underneath it on the overview
      page.</para>

      <para>Once created, refinements can be either removed or edited.</para>

      <para>Choosing the edit option on an override brings the user to the
      criteria page. Aside from the criteria tab replacing the refinements
      tab, the second level tabs will have the same options as they do for
      overrides and their behavior will be the same. Clicking the 'Done with
      refinement' tab will bring the user back to the refinements overview
      page.</para>

      <sect3>
        <title>Criteria</title>

        <para>The criteria page displays a list of the current criteria and
        the option to add any of the available criteria types to the list. It
        is also possible to remove existing criteria.</para>

        <note>
          <para>URIs must match <emphasis role="bold">all</emphasis> set
          criteria for the refinement to take effect.</para>
        </note>

        <para>Currently the following criteria can be applied:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Port number</emphasis></para>

            <para>Match only those URIs for the given port number.</para>

            <para>Default port number for HTTP is 80 and for 443 HTTPS.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Time of day</emphasis></para>

            <para>If this criteria is applied the refinement will be in effect
            between the hours specified each day.</para>

            <para>The format for the input boxes is HHMM (hours and
            minutes).</para>

            <para>An example might be: From 0200, To 0600. This refinement
            would be in effect between 2 and 6 am each night. Possibly to ease
            the politeness requirements during these hours when load on
            websites is generally low.<note>
                <para>As with all times in Heritrix these are <emphasis
                role="bold">always GMT</emphasis> times.</para>
              </note></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Regular expression</emphasis></para>

            <para>The refinement will only be in effect for those URIs that
            match the given regular expression.<note>
                <para>See <xref linkend="regexpr" /> for more on them.</para>
              </note></para>
          </listitem>
        </itemizedlist>
      </sect3>
    </sect2>
  </sect1>

  <sect1 id="running">
    <title>Running a job</title>

    <para>Once a crawl job has been created and properly configured it can be
    run. To start a crawl the user must go to the Console page (via the
    Console tab).</para>

    <sect2>
      <title>Console</title>

      <para>The Console presents on overview of the current status of the
      crawler.</para>

      <sect3>
        <title>Crawler status</title>

        <para>Regardless of if there is job being crawled the following
        information is provided:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Crawler running</emphasis></para>

            <para>Is the crawler running? If yes then the next pending job
            will be crawled as soon as the current one is completed. For more
            detail see <xref linkend="runningvcrawling" />.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Current job</emphasis></para>

            <para>The name of the job currently being crawled. If no job is
            being crawled this will read <emphasis>none</emphasis>.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Jobs pending</emphasis></para>

            <para>The number of jobs pending. That is waiting for their turn
            to be crawled.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Jobs completed</emphasis></para>

            <para>The number of jobs that have been crawled (including those
            that failed to start for some reason (see <xref
            linkend="failedtostart" /> for more on misconfigured jobs).</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Used memory</emphasis></para>

            <para>The amount of memory (in kilobytes) currently being used by
            the program.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Heap size</emphasis></para>

            <para>The amount of memory (in kilobytes) that the JavaVM has
            allocated itself.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Max heap size</emphasis></para>

            <para>The amount of memory (in kilobytes) that the JavaVM can
            <emphasis>at most </emphasis>allocate itself.</para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Alerts</emphasis></para>

            <para>Total number of alerts, and within brackets new alerts, if
            any.</para>

            <para>See <xref linkend="alerts" /> for more on alerts.</para>
          </listitem>
        </itemizedlist>

        <para>If a job is being crawled the following information is also
        provided:</para>

        <itemizedlist>
          <listitem>
            <para><emphasis role="bold">Status</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Processed docs/sec</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Processed KB/sec</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Run time</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Active thread count</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Total data written</emphasis></para>

            <para></para>
          </listitem>

          <listitem>
            <para><emphasis role="bold">Downloaded/Queued document
            ratio</emphasis></para>

            <para></para>
          </listitem>
        </itemizedlist>

        <para></para>

        <para>Some of this information is replicated in the head of each page
        (see <xref linkend="header" />).</para>

        <para></para>
      </sect3>

      <sect3>
        <title>Options</title>

        <para>Below the status display is a series of options. The exact
        options available depend on whether the crawler is running or not, and
        if so if there is a job being crawled and whether or not that job is
        paused or not.</para>

        <sect4>
          <title>Start / Stop crawling pending job</title>

          <para>If the crawler is running the option to start it is afforded.
          If it is already running the option to stop is provided.</para>

          <para>If the crawler is stopped, a job currently running will
          continue to run until it finishes. Stopping the crawler thus only
          prevents the next job from being run, it does not affect the current
          job. A separate option is provided for terminating the current
          job.<note>
              <para>See <xref linkend="runningvcrawling" /> for more on those
              terms.</para>
            </note></para>
        </sect4>

        <sect4>
          <title>Terminate current job</title>

          <para>Terminates the current job. The job's status will be marked as
          'Aborted by user'.</para>

          <para>All currently active threads will be allowed to finish behind
          the scenes even though the WUI will report the job being terminated
          at once.</para>

          <para>The next job (if one is pending) will start immediately
          despite this.</para>
        </sect4>

        <sect4>
          <title>Pause / Resume current job</title>

          <para>Issue a pause or resume (if already paused) order to the crawl
          in progress. Once the order to pause is issued it may take some time
          for all the active threads to enter a paused state. Until then the
          job is considered to be still running and 'waiting to pause'. It is
          possible to resume from that waiting state.</para>

          <para>Once paused the job is considered to be suspended and time
          spent in that state does not count towards the total time spent
          crawling that is used to calculate document per second rates
          etc.</para>
        </sect4>

        <sect4>
          <title>Inspect frontier URIs</title>

          <para>Only available for paused jobs (and only for those that
          <emphasis>have</emphasis> paused, not those that have been told to
          pause and are waiting for threads tor finish).</para>

          <para>Takes the user to a page that allows him to lookup URIs in the
          frontier and to delete them by using a regular expression.</para>
        </sect4>

        <sect4>
          <title>Refresh</title>

          <para>Update the status display. The status display does not update
          itself and quickly becomes out of date as crawling proceeds. This
          also refreshes the options available if they've changed as a result
          of a change in the state of the job being crawled.</para>
        </sect4>

        <sect4>
          <title>Shut down Heritrix</title>

          <para>It is possible to shut down Heritrix through this option.
          Doing so will terminate the Java process running Heritrix and the
          only way to start it up again will be via command line as this also
          disables the WUI.</para>

          <para>The user is asked to confirm this action twice to prevent
          accidental shut downs.</para>

          <para>This option will try to terminate any current job gracefully
          but will only wait a very short time for active threads to
          finish.</para>
        </sect4>
      </sect3>
    </sect2>

    <sect2>
      <title>Pending jobs</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Monitoring a running job</title>

      <para></para>

      <sect3 id="failedtostart">
        <title>Job failed to start</title>

        <para></para>
      </sect3>

      <sect3 id="header">
        <title>All page status header</title>

        <para></para>
      </sect3>
    </sect2>

    <sect2>
      <title>Editing a running job</title>

      <para></para>

      <sect3>
        <title>Journal</title>

        <para></para>
      </sect3>
    </sect2>
  </sect1>

  <sect1>
    <title>Analysis of jobs</title>

    <para></para>

    <sect2>
      <title>Completed jobs</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Logs</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Reports</title>

      <para></para>
    </sect2>

    <sect2>
      <title>Outside the user interface</title>

      <para></para>
    </sect2>
  </sect1>

  <sect1>
    <title>Worth noting</title>

    <para>The following chapters cover a few points that are worth noting as
    they may otherwise be confusing.</para>

    <sect2 id="runningvcrawling">
      <title>Running Vs. Crawling</title>

      <para>The state <emphasis>running</emphasis> generally means that the
      crawler will start executing a job as soon as one is made available in
      the pending jobs queue (as long as there is not a job currently being
      run).</para>

      <para>If the crawler is not in the running state, jobs added to the
      pending jobs queue will be held there in stasis; they will not be run,
      even if there are no jobs currently being run.</para>

      <para>The term <emphasis>crawling</emphasis> generally refers to a state
      whereby a job's being currently run (crawled): i.e. pages are being
      fetched, links extracted etc.</para>
    </sect2>

    <sect2>
      <title>Regular expressions</title>

      <para>All regular expressions used by Heritrix are Java regular
      expressions.</para>

      <para>Java regular expressions differ from those used in Perl, for
      example, in several ways. For detailed info on Java regular expressions
      see the Java API for java.util.regex.Pattern on Sun's home page (<ulink
      url="???">java.sun.com</ulink>).</para>

      <para>For API of Java SE v1.4.2 see <ulink
      url="http://java.sun.com/j2se/1.4.2/docs/api/index.html">http://java.sun.com/j2se/1.4.2/docs/api/index.html</ulink>,
      it is recommended you lookup the API for the version of Java that is
      being used to run Heritrix.</para>
    </sect2>

    <sect2>
      <title>Dates and times</title>

      <para>All times in Heritrix are GMT assuming the clock on the machine
      Heritrix is running on is correct.</para>

      <para>This means that all date/time stamps in logs are GMT, all dates
      and times shown in the WUI are GMT and any times or dates entered by the
      user need to be in GMT.</para>
    </sect2>

    <sect2>
      <title>Bytes, KB and statistics</title>

      <para></para>
    </sect2>

    <sect2 id="alerts">
      <title>Alerts</title>

      <para></para>
    </sect2>
  </sect1>
</article>